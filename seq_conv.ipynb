{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('tf': conda)",
   "metadata": {
    "interpreter": {
     "hash": "1d0891ab34821370daab934c4cb7710124cf36e69e82f6e017a5c665d1619cd9"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.layers as Layers\n",
    "#import tensorflow.keras.activations as Actications\n",
    "import tensorflow.keras.models as Models\n",
    "import tensorflow.keras.optimizers as Optimizer\n",
    "import tensorflow.keras.metrics as Metrics\n",
    "import tensorflow.keras.utils as Utils\n",
    "import tensorflow.keras.callbacks as Callbacks\n",
    "#from tensorflow.python.keras.utils.vis_utils import model_to_dot\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "#from sklearn.metrics import confusion_matrix as CM\n",
    "#from random import randint\n",
    "#from IPython.display import SVG\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(directory):\n",
    "    Images = []\n",
    "    Labels = []  # 0 for Building , 1 for forest, 2 for glacier, 3 for mountain, 4 for Sea , 5 for Street\n",
    "    label = 0\n",
    "    \n",
    "    for labels in os.listdir(directory): #Main Directory where each class label is present as folder name.\n",
    "        if labels == 'glacier': #Folder contain Glacier Images get the '2' class label.\n",
    "            label = 2\n",
    "        elif labels == 'sea':\n",
    "            label = 4\n",
    "        elif labels == 'buildings':\n",
    "            label = 0\n",
    "        elif labels == 'forest':\n",
    "            label = 1\n",
    "        elif labels == 'street':\n",
    "            label = 5\n",
    "        elif labels == 'mountain':\n",
    "            label = 3\n",
    "        \n",
    "        for image_file in os.listdir(directory+labels): #Extracting the file name of the image from Class Label folder\n",
    "            image = cv2.imread(directory+labels+r'/'+image_file) #Reading the image (OpenCV)\n",
    "            image = cv2.resize(image,(150,150)) #Resize the image, Some images are different sizes. (Resizing is very Important)\n",
    "            Images.append(image)\n",
    "            Labels.append(label)\n",
    "\n",
    "    return shuffle(Images,Labels,random_state=817328462) #Shuffle the dataset you just prepared.\n",
    "\n",
    "def get_classlabel(class_code):\n",
    "    labels = {2:'glacier', 4:'sea', 0:'buildings', 1:'forest', 5:'street', 3:'mountain'}\n",
    "    \n",
    "    return labels[class_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 14034 images belonging to 6 classes.\nFound 3000 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "train_DIR = \"../archive/seg_train/seg_train/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator( rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_DIR,\n",
    "                                                    batch_size=32,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=(150, 150))\n",
    "\n",
    "test_DIR = \"../archive/seg_test/seg_test/\"\n",
    "validation_datagen = ImageDataGenerator(rescale = 1.0/255)\n",
    "\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(test_DIR,\n",
    "                                                    batch_size=128,\n",
    "                                                    class_mode='categorical',\n",
    "                                                    target_size=(150, 150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Images, Labels = get_images('../archive/seg_train/seg_train/') #Extract the training images from the folders.\n",
    "\n",
    "# Images = np.array(Images) #converting the list of images to numpy array.\n",
    "# Labels = np.array(Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_labels = get_images('../archive/seg_test/seg_test/')\n",
    "# test_images = np.array(test_images)\n",
    "# test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BarContainer object of 6 artists>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 375.2875 248.518125\" width=\"375.2875pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-03T19:27:08.591092</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 375.2875 248.518125 \r\nL 375.2875 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\nL 368.0875 7.2 \r\nL 33.2875 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path clip-path=\"url(#p952b05cb58)\" d=\"M 48.505682 224.64 \r\nL 90.486873 224.64 \r\nL 90.486873 60.993604 \r\nL 48.505682 60.993604 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path clip-path=\"url(#p952b05cb58)\" d=\"M 100.982171 224.64 \r\nL 142.963362 224.64 \r\nL 142.963362 47.137959 \r\nL 100.982171 47.137959 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path clip-path=\"url(#p952b05cb58)\" d=\"M 153.45866 224.64 \r\nL 195.439851 224.64 \r\nL 195.439851 17.554286 \r\nL 153.45866 17.554286 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path clip-path=\"url(#p952b05cb58)\" d=\"M 205.935149 224.64 \r\nL 247.91634 224.64 \r\nL 247.91634 28.039638 \r\nL 205.935149 28.039638 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path clip-path=\"url(#p952b05cb58)\" d=\"M 258.411638 224.64 \r\nL 300.392829 224.64 \r\nL 300.392829 33.656792 \r\nL 258.411638 33.656792 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_8\">\r\n    <path clip-path=\"url(#p952b05cb58)\" d=\"M 310.888127 224.64 \r\nL 352.869318 224.64 \r\nL 352.869318 37.027083 \r\nL 310.888127 37.027083 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"mb45aa45434\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.496277\" xlink:href=\"#mb45aa45434\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- mountain -->\r\n      <g transform=\"translate(45.647059 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n        <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n        <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n        <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n        <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n        <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n        <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-109\"/>\r\n       <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"158.59375\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"221.972656\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"285.351562\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"324.560547\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"385.839844\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"413.623047\" xlink:href=\"#DejaVuSans-110\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"121.972766\" xlink:href=\"#mb45aa45434\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- street -->\r\n      <g transform=\"translate(107.351673 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n        <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n        <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\r\n       <use x=\"91.308594\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"130.171875\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"191.695312\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"253.21875\" xlink:href=\"#DejaVuSans-116\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"174.449255\" xlink:href=\"#mb45aa45434\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- glacier -->\r\n      <g transform=\"translate(157.551599 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n        <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n        <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"91.259766\" xlink:href=\"#DejaVuSans-97\"/>\r\n       <use x=\"152.539062\" xlink:href=\"#DejaVuSans-99\"/>\r\n       <use x=\"207.519531\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"235.302734\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"296.826172\" xlink:href=\"#DejaVuSans-114\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"226.925745\" xlink:href=\"#mb45aa45434\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- buildings -->\r\n      <g transform=\"translate(204.293713 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n        <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-98\"/>\r\n       <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n       <use x=\"126.855469\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\r\n       <use x=\"182.421875\" xlink:href=\"#DejaVuSans-100\"/>\r\n       <use x=\"245.898438\" xlink:href=\"#DejaVuSans-105\"/>\r\n       <use x=\"273.681641\" xlink:href=\"#DejaVuSans-110\"/>\r\n       <use x=\"337.060547\" xlink:href=\"#DejaVuSans-103\"/>\r\n       <use x=\"400.537109\" xlink:href=\"#DejaVuSans-115\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"279.402234\" xlink:href=\"#mb45aa45434\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- sea -->\r\n      <g transform=\"translate(270.656921 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"113.623047\" xlink:href=\"#DejaVuSans-97\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"331.878723\" xlink:href=\"#mb45aa45434\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- forest -->\r\n      <g transform=\"translate(317.474816 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-102\"/>\r\n       <use x=\"35.205078\" xlink:href=\"#DejaVuSans-111\"/>\r\n       <use x=\"96.386719\" xlink:href=\"#DejaVuSans-114\"/>\r\n       <use x=\"135.25\" xlink:href=\"#DejaVuSans-101\"/>\r\n       <use x=\"196.773438\" xlink:href=\"#DejaVuSans-115\"/>\r\n       <use x=\"248.873047\" xlink:href=\"#DejaVuSans-116\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mc40d620494\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc40d620494\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(19.925 228.439219)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc40d620494\" y=\"187.192312\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(7.2 190.991531)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc40d620494\" y=\"149.744624\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(7.2 153.543843)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc40d620494\" y=\"112.296936\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(7.2 116.096155)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc40d620494\" y=\"74.849248\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(7.2 78.648467)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"33.2875\" xlink:href=\"#mc40d620494\" y=\"37.40156\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 500 -->\r\n      <g transform=\"translate(7.2 41.200779)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_9\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 33.2875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_10\">\r\n    <path d=\"M 368.0875 224.64 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_11\">\r\n    <path d=\"M 33.2875 224.64 \r\nL 368.0875 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_12\">\r\n    <path d=\"M 33.2875 7.2 \r\nL 368.0875 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p952b05cb58\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"33.2875\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "test_counts = np.unique(test_labels)\n",
    "chart = np.array([test_labels.count(n) for n in test_counts])\n",
    "plt.bar(class_names,chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['mountain', 'street', 'glacier', 'buildings', 'sea', 'forest']\n",
    "class_names_label = {class_name:i for i, class_name in enumerate(class_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"246.958125pt\" version=\"1.1\" viewBox=\"0 0 279.854592 246.958125\" width=\"279.854592pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2020-12-03T19:28:18.759832</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.3.2, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 246.958125 \r\nL 279.854592 246.958125 \r\nL 279.854592 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 226.102967 130.653848 \r\nC 226.102967 117.328761 223.040778 104.180186 217.153501 92.226195 \r\nC 211.266223 80.272203 202.70983 69.829495 192.146828 61.70663 \r\nL 139.126967 130.653848 \r\nL 226.102967 130.653848 \r\nz\r\n\" style=\"fill:#1f77b4;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 191.504422 61.232142 \r\nC 180.038376 52.414844 166.551904 46.595487 152.270488 44.302863 \r\nC 137.989073 42.010238 123.358798 43.315957 109.709081 48.101371 \r\nL 138.484561 130.17936 \r\nL 191.504422 61.232142 \r\nz\r\n\" style=\"fill:#ff7f0e;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 108.881195 48.430659 \r\nC 92.927552 54.0238 78.945491 64.148845 68.653968 77.561022 \r\nC 58.362446 90.973199 52.201229 107.099389 50.927781 123.957036 \r\nL 137.656674 130.508649 \r\nL 108.881195 48.430659 \r\nz\r\n\" style=\"fill:#2ca02c;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 50.844091 124.883807 \r\nC 49.635936 140.877104 52.874884 156.897188 60.201237 171.164974 \r\nC 67.52759 185.432759 78.659718 197.399781 92.361359 205.737059 \r\nL 137.572984 131.435421 \r\nL 50.844091 124.883807 \r\nz\r\n\" style=\"fill:#d62728;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 93.124147 206.209455 \r\nC 106.429685 214.30571 121.668874 218.681232 137.242838 218.876949 \r\nC 152.816802 219.072666 168.161135 215.081484 181.665929 207.322152 \r\nL 138.335772 131.907816 \r\nL 93.124147 206.209455 \r\nz\r\n\" style=\"fill:#9467bd;\"/>\r\n   </g>\r\n   <g id=\"patch_7\">\r\n    <path d=\"M 182.429637 206.88813 \r\nC 195.693403 199.267283 206.714853 188.280402 214.377328 175.04064 \r\nC 222.039804 161.800878 226.075481 146.771001 226.07548 131.473785 \r\nL 139.09948 131.473793 \r\nL 182.429637 206.88813 \r\nz\r\n\" style=\"fill:#8c564b;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\"/>\r\n   <g id=\"matplotlib.axis_2\"/>\r\n   <g id=\"text_1\">\r\n    <!-- mountain -->\r\n    <g transform=\"translate(224.956154 91.142805)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 52 44.1875 \r\nQ 55.375 50.25 60.0625 53.125 \r\nQ 64.75 56 71.09375 56 \r\nQ 79.640625 56 84.28125 50.015625 \r\nQ 88.921875 44.046875 88.921875 33.015625 \r\nL 88.921875 0 \r\nL 79.890625 0 \r\nL 79.890625 32.71875 \r\nQ 79.890625 40.578125 77.09375 44.375 \r\nQ 74.3125 48.1875 68.609375 48.1875 \r\nQ 61.625 48.1875 57.5625 43.546875 \r\nQ 53.515625 38.921875 53.515625 30.90625 \r\nL 53.515625 0 \r\nL 44.484375 0 \r\nL 44.484375 32.71875 \r\nQ 44.484375 40.625 41.703125 44.40625 \r\nQ 38.921875 48.1875 33.109375 48.1875 \r\nQ 26.21875 48.1875 22.15625 43.53125 \r\nQ 18.109375 38.875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.1875 51.21875 25.484375 53.609375 \r\nQ 29.78125 56 35.6875 56 \r\nQ 41.65625 56 45.828125 52.96875 \r\nQ 50 49.953125 52 44.1875 \r\nz\r\n\" id=\"DejaVuSans-109\"/>\r\n      <path d=\"M 30.609375 48.390625 \r\nQ 23.390625 48.390625 19.1875 42.75 \r\nQ 14.984375 37.109375 14.984375 27.296875 \r\nQ 14.984375 17.484375 19.15625 11.84375 \r\nQ 23.34375 6.203125 30.609375 6.203125 \r\nQ 37.796875 6.203125 41.984375 11.859375 \r\nQ 46.1875 17.53125 46.1875 27.296875 \r\nQ 46.1875 37.015625 41.984375 42.703125 \r\nQ 37.796875 48.390625 30.609375 48.390625 \r\nz\r\nM 30.609375 56 \r\nQ 42.328125 56 49.015625 48.375 \r\nQ 55.71875 40.765625 55.71875 27.296875 \r\nQ 55.71875 13.875 49.015625 6.21875 \r\nQ 42.328125 -1.421875 30.609375 -1.421875 \r\nQ 18.84375 -1.421875 12.171875 6.21875 \r\nQ 5.515625 13.875 5.515625 27.296875 \r\nQ 5.515625 40.765625 12.171875 48.375 \r\nQ 18.84375 56 30.609375 56 \r\nz\r\n\" id=\"DejaVuSans-111\"/>\r\n      <path d=\"M 8.5 21.578125 \r\nL 8.5 54.6875 \r\nL 17.484375 54.6875 \r\nL 17.484375 21.921875 \r\nQ 17.484375 14.15625 20.5 10.265625 \r\nQ 23.53125 6.390625 29.59375 6.390625 \r\nQ 36.859375 6.390625 41.078125 11.03125 \r\nQ 45.3125 15.671875 45.3125 23.6875 \r\nL 45.3125 54.6875 \r\nL 54.296875 54.6875 \r\nL 54.296875 0 \r\nL 45.3125 0 \r\nL 45.3125 8.40625 \r\nQ 42.046875 3.421875 37.71875 1 \r\nQ 33.40625 -1.421875 27.6875 -1.421875 \r\nQ 18.265625 -1.421875 13.375 4.4375 \r\nQ 8.5 10.296875 8.5 21.578125 \r\nz\r\nM 31.109375 56 \r\nz\r\n\" id=\"DejaVuSans-117\"/>\r\n      <path d=\"M 54.890625 33.015625 \r\nL 54.890625 0 \r\nL 45.90625 0 \r\nL 45.90625 32.71875 \r\nQ 45.90625 40.484375 42.875 44.328125 \r\nQ 39.84375 48.1875 33.796875 48.1875 \r\nQ 26.515625 48.1875 22.3125 43.546875 \r\nQ 18.109375 38.921875 18.109375 30.90625 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 21.34375 51.125 25.703125 53.5625 \r\nQ 30.078125 56 35.796875 56 \r\nQ 45.21875 56 50.046875 50.171875 \r\nQ 54.890625 44.34375 54.890625 33.015625 \r\nz\r\n\" id=\"DejaVuSans-110\"/>\r\n      <path d=\"M 18.3125 70.21875 \r\nL 18.3125 54.6875 \r\nL 36.8125 54.6875 \r\nL 36.8125 47.703125 \r\nL 18.3125 47.703125 \r\nL 18.3125 18.015625 \r\nQ 18.3125 11.328125 20.140625 9.421875 \r\nQ 21.96875 7.515625 27.59375 7.515625 \r\nL 36.8125 7.515625 \r\nL 36.8125 0 \r\nL 27.59375 0 \r\nQ 17.1875 0 13.234375 3.875 \r\nQ 9.28125 7.765625 9.28125 18.015625 \r\nL 9.28125 47.703125 \r\nL 2.6875 47.703125 \r\nL 2.6875 54.6875 \r\nL 9.28125 54.6875 \r\nL 9.28125 70.21875 \r\nz\r\n\" id=\"DejaVuSans-116\"/>\r\n      <path d=\"M 34.28125 27.484375 \r\nQ 23.390625 27.484375 19.1875 25 \r\nQ 14.984375 22.515625 14.984375 16.5 \r\nQ 14.984375 11.71875 18.140625 8.90625 \r\nQ 21.296875 6.109375 26.703125 6.109375 \r\nQ 34.1875 6.109375 38.703125 11.40625 \r\nQ 43.21875 16.703125 43.21875 25.484375 \r\nL 43.21875 27.484375 \r\nz\r\nM 52.203125 31.203125 \r\nL 52.203125 0 \r\nL 43.21875 0 \r\nL 43.21875 8.296875 \r\nQ 40.140625 3.328125 35.546875 0.953125 \r\nQ 30.953125 -1.421875 24.3125 -1.421875 \r\nQ 15.921875 -1.421875 10.953125 3.296875 \r\nQ 6 8.015625 6 15.921875 \r\nQ 6 25.140625 12.171875 29.828125 \r\nQ 18.359375 34.515625 30.609375 34.515625 \r\nL 43.21875 34.515625 \r\nL 43.21875 35.40625 \r\nQ 43.21875 41.609375 39.140625 45 \r\nQ 35.0625 48.390625 27.6875 48.390625 \r\nQ 23 48.390625 18.546875 47.265625 \r\nQ 14.109375 46.140625 10.015625 43.890625 \r\nL 10.015625 52.203125 \r\nQ 14.9375 54.109375 19.578125 55.046875 \r\nQ 24.21875 56 28.609375 56 \r\nQ 40.484375 56 46.34375 49.84375 \r\nQ 52.203125 43.703125 52.203125 31.203125 \r\nz\r\n\" id=\"DejaVuSans-97\"/>\r\n      <path d=\"M 9.421875 54.6875 \r\nL 18.40625 54.6875 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\nM 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 64.59375 \r\nL 9.421875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-105\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-109\"/>\r\n     <use x=\"97.412109\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"158.59375\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"221.972656\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"285.351562\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"324.560547\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"385.839844\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"413.623047\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_2\">\r\n    <!-- 14.6% -->\r\n    <g transform=\"translate(170.059293 110.356631)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n      <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      <path d=\"M 72.703125 32.078125 \r\nQ 68.453125 32.078125 66.03125 28.46875 \r\nQ 63.625 24.859375 63.625 18.40625 \r\nQ 63.625 12.0625 66.03125 8.421875 \r\nQ 68.453125 4.78125 72.703125 4.78125 \r\nQ 76.859375 4.78125 79.265625 8.421875 \r\nQ 81.6875 12.0625 81.6875 18.40625 \r\nQ 81.6875 24.8125 79.265625 28.4375 \r\nQ 76.859375 32.078125 72.703125 32.078125 \r\nz\r\nM 72.703125 38.28125 \r\nQ 80.421875 38.28125 84.953125 32.90625 \r\nQ 89.5 27.546875 89.5 18.40625 \r\nQ 89.5 9.28125 84.9375 3.921875 \r\nQ 80.375 -1.421875 72.703125 -1.421875 \r\nQ 64.890625 -1.421875 60.34375 3.921875 \r\nQ 55.8125 9.28125 55.8125 18.40625 \r\nQ 55.8125 27.59375 60.375 32.9375 \r\nQ 64.9375 38.28125 72.703125 38.28125 \r\nz\r\nM 22.3125 68.015625 \r\nQ 18.109375 68.015625 15.6875 64.375 \r\nQ 13.28125 60.75 13.28125 54.390625 \r\nQ 13.28125 47.953125 15.671875 44.328125 \r\nQ 18.0625 40.71875 22.3125 40.71875 \r\nQ 26.5625 40.71875 28.96875 44.328125 \r\nQ 31.390625 47.953125 31.390625 54.390625 \r\nQ 31.390625 60.6875 28.953125 64.34375 \r\nQ 26.515625 68.015625 22.3125 68.015625 \r\nz\r\nM 66.40625 74.21875 \r\nL 74.21875 74.21875 \r\nL 28.609375 -1.421875 \r\nL 20.796875 -1.421875 \r\nz\r\nM 22.3125 74.21875 \r\nQ 30.03125 74.21875 34.609375 68.875 \r\nQ 39.203125 63.53125 39.203125 54.390625 \r\nQ 39.203125 45.171875 34.640625 39.84375 \r\nQ 30.078125 34.515625 22.3125 34.515625 \r\nQ 14.546875 34.515625 10.03125 39.859375 \r\nQ 5.515625 45.21875 5.515625 54.390625 \r\nQ 5.515625 63.484375 10.046875 68.84375 \r\nQ 14.59375 74.21875 22.3125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-37\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-52\"/>\r\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-54\"/>\r\n     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_3\">\r\n    <!-- street -->\r\n    <g transform=\"translate(153.649081 38.474588)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 44.28125 53.078125 \r\nL 44.28125 44.578125 \r\nQ 40.484375 46.53125 36.375 47.5 \r\nQ 32.28125 48.484375 27.875 48.484375 \r\nQ 21.1875 48.484375 17.84375 46.4375 \r\nQ 14.5 44.390625 14.5 40.28125 \r\nQ 14.5 37.15625 16.890625 35.375 \r\nQ 19.28125 33.59375 26.515625 31.984375 \r\nL 29.59375 31.296875 \r\nQ 39.15625 29.25 43.1875 25.515625 \r\nQ 47.21875 21.78125 47.21875 15.09375 \r\nQ 47.21875 7.46875 41.1875 3.015625 \r\nQ 35.15625 -1.421875 24.609375 -1.421875 \r\nQ 20.21875 -1.421875 15.453125 -0.5625 \r\nQ 10.6875 0.296875 5.421875 2 \r\nL 5.421875 11.28125 \r\nQ 10.40625 8.6875 15.234375 7.390625 \r\nQ 20.0625 6.109375 24.8125 6.109375 \r\nQ 31.15625 6.109375 34.5625 8.28125 \r\nQ 37.984375 10.453125 37.984375 14.40625 \r\nQ 37.984375 18.0625 35.515625 20.015625 \r\nQ 33.0625 21.96875 24.703125 23.78125 \r\nL 21.578125 24.515625 \r\nQ 13.234375 26.265625 9.515625 29.90625 \r\nQ 5.8125 33.546875 5.8125 39.890625 \r\nQ 5.8125 47.609375 11.28125 51.796875 \r\nQ 16.75 56 26.8125 56 \r\nQ 31.78125 56 36.171875 55.265625 \r\nQ 40.578125 54.546875 44.28125 53.078125 \r\nz\r\n\" id=\"DejaVuSans-115\"/>\r\n      <path d=\"M 41.109375 46.296875 \r\nQ 39.59375 47.171875 37.8125 47.578125 \r\nQ 36.03125 48 33.890625 48 \r\nQ 26.265625 48 22.1875 43.046875 \r\nQ 18.109375 38.09375 18.109375 28.8125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 54.6875 \r\nL 18.109375 54.6875 \r\nL 18.109375 46.1875 \r\nQ 20.953125 51.171875 25.484375 53.578125 \r\nQ 30.03125 56 36.53125 56 \r\nQ 37.453125 56 38.578125 55.875 \r\nQ 39.703125 55.765625 41.0625 55.515625 \r\nz\r\n\" id=\"DejaVuSans-114\"/>\r\n      <path d=\"M 56.203125 29.59375 \r\nL 56.203125 25.203125 \r\nL 14.890625 25.203125 \r\nQ 15.484375 15.921875 20.484375 11.0625 \r\nQ 25.484375 6.203125 34.421875 6.203125 \r\nQ 39.59375 6.203125 44.453125 7.46875 \r\nQ 49.3125 8.734375 54.109375 11.28125 \r\nL 54.109375 2.78125 \r\nQ 49.265625 0.734375 44.1875 -0.34375 \r\nQ 39.109375 -1.421875 33.890625 -1.421875 \r\nQ 20.796875 -1.421875 13.15625 6.1875 \r\nQ 5.515625 13.8125 5.515625 26.8125 \r\nQ 5.515625 40.234375 12.765625 48.109375 \r\nQ 20.015625 56 32.328125 56 \r\nQ 43.359375 56 49.78125 48.890625 \r\nQ 56.203125 41.796875 56.203125 29.59375 \r\nz\r\nM 47.21875 32.234375 \r\nQ 47.125 39.59375 43.09375 43.984375 \r\nQ 39.0625 48.390625 32.421875 48.390625 \r\nQ 24.90625 48.390625 20.390625 44.140625 \r\nQ 15.875 39.890625 15.1875 32.171875 \r\nz\r\n\" id=\"DejaVuSans-101\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"91.308594\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"130.171875\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"191.695312\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"253.21875\" xlink:href=\"#DejaVuSans-116\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_4\">\r\n    <!-- 15.8% -->\r\n    <g transform=\"translate(130.872523 81.412837)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\r\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-56\"/>\r\n     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_5\">\r\n    <!-- glacier -->\r\n    <g transform=\"translate(27.958385 75.025635)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 45.40625 27.984375 \r\nQ 45.40625 37.75 41.375 43.109375 \r\nQ 37.359375 48.484375 30.078125 48.484375 \r\nQ 22.859375 48.484375 18.828125 43.109375 \r\nQ 14.796875 37.75 14.796875 27.984375 \r\nQ 14.796875 18.265625 18.828125 12.890625 \r\nQ 22.859375 7.515625 30.078125 7.515625 \r\nQ 37.359375 7.515625 41.375 12.890625 \r\nQ 45.40625 18.265625 45.40625 27.984375 \r\nz\r\nM 54.390625 6.78125 \r\nQ 54.390625 -7.171875 48.1875 -13.984375 \r\nQ 42 -20.796875 29.203125 -20.796875 \r\nQ 24.46875 -20.796875 20.265625 -20.09375 \r\nQ 16.0625 -19.390625 12.109375 -17.921875 \r\nL 12.109375 -9.1875 \r\nQ 16.0625 -11.328125 19.921875 -12.34375 \r\nQ 23.78125 -13.375 27.78125 -13.375 \r\nQ 36.625 -13.375 41.015625 -8.765625 \r\nQ 45.40625 -4.15625 45.40625 5.171875 \r\nL 45.40625 9.625 \r\nQ 42.625 4.78125 38.28125 2.390625 \r\nQ 33.9375 0 27.875 0 \r\nQ 17.828125 0 11.671875 7.65625 \r\nQ 5.515625 15.328125 5.515625 27.984375 \r\nQ 5.515625 40.671875 11.671875 48.328125 \r\nQ 17.828125 56 27.875 56 \r\nQ 33.9375 56 38.28125 53.609375 \r\nQ 42.625 51.21875 45.40625 46.390625 \r\nL 45.40625 54.6875 \r\nL 54.390625 54.6875 \r\nz\r\n\" id=\"DejaVuSans-103\"/>\r\n      <path d=\"M 9.421875 75.984375 \r\nL 18.40625 75.984375 \r\nL 18.40625 0 \r\nL 9.421875 0 \r\nz\r\n\" id=\"DejaVuSans-108\"/>\r\n      <path d=\"M 48.78125 52.59375 \r\nL 48.78125 44.1875 \r\nQ 44.96875 46.296875 41.140625 47.34375 \r\nQ 37.3125 48.390625 33.40625 48.390625 \r\nQ 24.65625 48.390625 19.8125 42.84375 \r\nQ 14.984375 37.3125 14.984375 27.296875 \r\nQ 14.984375 17.28125 19.8125 11.734375 \r\nQ 24.65625 6.203125 33.40625 6.203125 \r\nQ 37.3125 6.203125 41.140625 7.25 \r\nQ 44.96875 8.296875 48.78125 10.40625 \r\nL 48.78125 2.09375 \r\nQ 45.015625 0.34375 40.984375 -0.53125 \r\nQ 36.96875 -1.421875 32.421875 -1.421875 \r\nQ 20.0625 -1.421875 12.78125 6.34375 \r\nQ 5.515625 14.109375 5.515625 27.296875 \r\nQ 5.515625 40.671875 12.859375 48.328125 \r\nQ 20.21875 56 33.015625 56 \r\nQ 37.15625 56 41.109375 55.140625 \r\nQ 45.0625 54.296875 48.78125 52.59375 \r\nz\r\n\" id=\"DejaVuSans-99\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"91.259766\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"152.539062\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"207.519531\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"235.302734\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"296.826172\" xlink:href=\"#DejaVuSans-114\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_6\">\r\n    <!-- 18.4% -->\r\n    <g transform=\"translate(80.371457 101.499448)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-56\"/>\r\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-52\"/>\r\n     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_7\">\r\n    <!-- buildings -->\r\n    <g transform=\"translate(7.2 177.897304)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 48.6875 27.296875 \r\nQ 48.6875 37.203125 44.609375 42.84375 \r\nQ 40.53125 48.484375 33.40625 48.484375 \r\nQ 26.265625 48.484375 22.1875 42.84375 \r\nQ 18.109375 37.203125 18.109375 27.296875 \r\nQ 18.109375 17.390625 22.1875 11.75 \r\nQ 26.265625 6.109375 33.40625 6.109375 \r\nQ 40.53125 6.109375 44.609375 11.75 \r\nQ 48.6875 17.390625 48.6875 27.296875 \r\nz\r\nM 18.109375 46.390625 \r\nQ 20.953125 51.265625 25.265625 53.625 \r\nQ 29.59375 56 35.59375 56 \r\nQ 45.5625 56 51.78125 48.09375 \r\nQ 58.015625 40.1875 58.015625 27.296875 \r\nQ 58.015625 14.40625 51.78125 6.484375 \r\nQ 45.5625 -1.421875 35.59375 -1.421875 \r\nQ 29.59375 -1.421875 25.265625 0.953125 \r\nQ 20.953125 3.328125 18.109375 8.203125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nL 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-98\"/>\r\n      <path d=\"M 45.40625 46.390625 \r\nL 45.40625 75.984375 \r\nL 54.390625 75.984375 \r\nL 54.390625 0 \r\nL 45.40625 0 \r\nL 45.40625 8.203125 \r\nQ 42.578125 3.328125 38.25 0.953125 \r\nQ 33.9375 -1.421875 27.875 -1.421875 \r\nQ 17.96875 -1.421875 11.734375 6.484375 \r\nQ 5.515625 14.40625 5.515625 27.296875 \r\nQ 5.515625 40.1875 11.734375 48.09375 \r\nQ 17.96875 56 27.875 56 \r\nQ 33.9375 56 38.25 53.625 \r\nQ 42.578125 51.265625 45.40625 46.390625 \r\nz\r\nM 14.796875 27.296875 \r\nQ 14.796875 17.390625 18.875 11.75 \r\nQ 22.953125 6.109375 30.078125 6.109375 \r\nQ 37.203125 6.109375 41.296875 11.75 \r\nQ 45.40625 17.390625 45.40625 27.296875 \r\nQ 45.40625 37.203125 41.296875 42.84375 \r\nQ 37.203125 48.484375 30.078125 48.484375 \r\nQ 22.953125 48.484375 18.875 42.84375 \r\nQ 14.796875 37.203125 14.796875 27.296875 \r\nz\r\n\" id=\"DejaVuSans-100\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-98\"/>\r\n     <use x=\"63.476562\" xlink:href=\"#DejaVuSans-117\"/>\r\n     <use x=\"126.855469\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"154.638672\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"182.421875\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"245.898438\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"273.681641\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"337.060547\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"400.537109\" xlink:href=\"#DejaVuSans-115\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_8\">\r\n    <!-- 17.5% -->\r\n    <g transform=\"translate(75.266342 158.032527)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\r\n     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_9\">\r\n    <!-- sea -->\r\n    <g transform=\"translate(119.64292 230.333237)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"52.099609\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"113.623047\" xlink:href=\"#DejaVuSans-97\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_10\">\r\n    <!-- 17.0% -->\r\n    <g transform=\"translate(121.796418 186.848671)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-55\"/>\r\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\r\n     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_11\">\r\n    <!-- forest -->\r\n    <g transform=\"translate(221.905113 182.1567)scale(0.1 -0.1)\">\r\n     <defs>\r\n      <path d=\"M 37.109375 75.984375 \r\nL 37.109375 68.5 \r\nL 28.515625 68.5 \r\nQ 23.6875 68.5 21.796875 66.546875 \r\nQ 19.921875 64.59375 19.921875 59.515625 \r\nL 19.921875 54.6875 \r\nL 34.71875 54.6875 \r\nL 34.71875 47.703125 \r\nL 19.921875 47.703125 \r\nL 19.921875 0 \r\nL 10.890625 0 \r\nL 10.890625 47.703125 \r\nL 2.296875 47.703125 \r\nL 2.296875 54.6875 \r\nL 10.890625 54.6875 \r\nL 10.890625 58.5 \r\nQ 10.890625 67.625 15.140625 71.796875 \r\nQ 19.390625 75.984375 28.609375 75.984375 \r\nz\r\n\" id=\"DejaVuSans-102\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-102\"/>\r\n     <use x=\"35.205078\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"96.386719\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"135.25\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"196.773438\" xlink:href=\"#DejaVuSans-115\"/>\r\n     <use x=\"248.873047\" xlink:href=\"#DejaVuSans-116\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_12\">\r\n    <!-- 16.7% -->\r\n    <g transform=\"translate(168.382595 160.373276)scale(0.1 -0.1)\">\r\n     <use xlink:href=\"#DejaVuSans-49\"/>\r\n     <use x=\"63.623047\" xlink:href=\"#DejaVuSans-54\"/>\r\n     <use x=\"127.246094\" xlink:href=\"#DejaVuSans-46\"/>\r\n     <use x=\"159.033203\" xlink:href=\"#DejaVuSans-55\"/>\r\n     <use x=\"222.65625\" xlink:href=\"#DejaVuSans-37\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"text_13\">\r\n    <!-- Training Categorical Breakdown -->\r\n    <g transform=\"translate(43.213889 16.318125)scale(0.12 -0.12)\">\r\n     <defs>\r\n      <path d=\"M -0.296875 72.90625 \r\nL 61.375 72.90625 \r\nL 61.375 64.59375 \r\nL 35.5 64.59375 \r\nL 35.5 0 \r\nL 25.59375 0 \r\nL 25.59375 64.59375 \r\nL -0.296875 64.59375 \r\nz\r\n\" id=\"DejaVuSans-84\"/>\r\n      <path id=\"DejaVuSans-32\"/>\r\n      <path d=\"M 64.40625 67.28125 \r\nL 64.40625 56.890625 \r\nQ 59.421875 61.53125 53.78125 63.8125 \r\nQ 48.140625 66.109375 41.796875 66.109375 \r\nQ 29.296875 66.109375 22.65625 58.46875 \r\nQ 16.015625 50.828125 16.015625 36.375 \r\nQ 16.015625 21.96875 22.65625 14.328125 \r\nQ 29.296875 6.6875 41.796875 6.6875 \r\nQ 48.140625 6.6875 53.78125 8.984375 \r\nQ 59.421875 11.28125 64.40625 15.921875 \r\nL 64.40625 5.609375 \r\nQ 59.234375 2.09375 53.4375 0.328125 \r\nQ 47.65625 -1.421875 41.21875 -1.421875 \r\nQ 24.65625 -1.421875 15.125 8.703125 \r\nQ 5.609375 18.84375 5.609375 36.375 \r\nQ 5.609375 53.953125 15.125 64.078125 \r\nQ 24.65625 74.21875 41.21875 74.21875 \r\nQ 47.75 74.21875 53.53125 72.484375 \r\nQ 59.328125 70.75 64.40625 67.28125 \r\nz\r\n\" id=\"DejaVuSans-67\"/>\r\n      <path d=\"M 19.671875 34.8125 \r\nL 19.671875 8.109375 \r\nL 35.5 8.109375 \r\nQ 43.453125 8.109375 47.28125 11.40625 \r\nQ 51.125 14.703125 51.125 21.484375 \r\nQ 51.125 28.328125 47.28125 31.5625 \r\nQ 43.453125 34.8125 35.5 34.8125 \r\nz\r\nM 19.671875 64.796875 \r\nL 19.671875 42.828125 \r\nL 34.28125 42.828125 \r\nQ 41.5 42.828125 45.03125 45.53125 \r\nQ 48.578125 48.25 48.578125 53.8125 \r\nQ 48.578125 59.328125 45.03125 62.0625 \r\nQ 41.5 64.796875 34.28125 64.796875 \r\nz\r\nM 9.8125 72.90625 \r\nL 35.015625 72.90625 \r\nQ 46.296875 72.90625 52.390625 68.21875 \r\nQ 58.5 63.53125 58.5 54.890625 \r\nQ 58.5 48.1875 55.375 44.234375 \r\nQ 52.25 40.28125 46.1875 39.3125 \r\nQ 53.46875 37.75 57.5 32.78125 \r\nQ 61.53125 27.828125 61.53125 20.40625 \r\nQ 61.53125 10.640625 54.890625 5.3125 \r\nQ 48.25 0 35.984375 0 \r\nL 9.8125 0 \r\nz\r\n\" id=\"DejaVuSans-66\"/>\r\n      <path d=\"M 9.078125 75.984375 \r\nL 18.109375 75.984375 \r\nL 18.109375 31.109375 \r\nL 44.921875 54.6875 \r\nL 56.390625 54.6875 \r\nL 27.390625 29.109375 \r\nL 57.625 0 \r\nL 45.90625 0 \r\nL 18.109375 26.703125 \r\nL 18.109375 0 \r\nL 9.078125 0 \r\nz\r\n\" id=\"DejaVuSans-107\"/>\r\n      <path d=\"M 4.203125 54.6875 \r\nL 13.1875 54.6875 \r\nL 24.421875 12.015625 \r\nL 35.59375 54.6875 \r\nL 46.1875 54.6875 \r\nL 57.421875 12.015625 \r\nL 68.609375 54.6875 \r\nL 77.59375 54.6875 \r\nL 63.28125 0 \r\nL 52.6875 0 \r\nL 40.921875 44.828125 \r\nL 29.109375 0 \r\nL 18.5 0 \r\nz\r\n\" id=\"DejaVuSans-119\"/>\r\n     </defs>\r\n     <use xlink:href=\"#DejaVuSans-84\"/>\r\n     <use x=\"46.333984\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"87.447266\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"148.726562\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"176.509766\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"239.888672\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"267.671875\" xlink:href=\"#DejaVuSans-110\"/>\r\n     <use x=\"331.050781\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"394.527344\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"426.314453\" xlink:href=\"#DejaVuSans-67\"/>\r\n     <use x=\"496.138672\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"557.417969\" xlink:href=\"#DejaVuSans-116\"/>\r\n     <use x=\"596.626953\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"658.150391\" xlink:href=\"#DejaVuSans-103\"/>\r\n     <use x=\"721.626953\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"782.808594\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"823.921875\" xlink:href=\"#DejaVuSans-105\"/>\r\n     <use x=\"851.705078\" xlink:href=\"#DejaVuSans-99\"/>\r\n     <use x=\"906.685547\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"967.964844\" xlink:href=\"#DejaVuSans-108\"/>\r\n     <use x=\"995.748047\" xlink:href=\"#DejaVuSans-32\"/>\r\n     <use x=\"1027.535156\" xlink:href=\"#DejaVuSans-66\"/>\r\n     <use x=\"1096.138672\" xlink:href=\"#DejaVuSans-114\"/>\r\n     <use x=\"1135.001953\" xlink:href=\"#DejaVuSans-101\"/>\r\n     <use x=\"1196.525391\" xlink:href=\"#DejaVuSans-97\"/>\r\n     <use x=\"1257.804688\" xlink:href=\"#DejaVuSans-107\"/>\r\n     <use x=\"1315.714844\" xlink:href=\"#DejaVuSans-100\"/>\r\n     <use x=\"1379.191406\" xlink:href=\"#DejaVuSans-111\"/>\r\n     <use x=\"1440.373047\" xlink:href=\"#DejaVuSans-119\"/>\r\n     <use x=\"1522.160156\" xlink:href=\"#DejaVuSans-110\"/>\r\n    </g>\r\n   </g>\r\n  </g>\r\n </g>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {}
    }
   ],
   "source": [
    "labels = {2:'glacier', 4:'sea', 0:'buildings', 1:'forest', 5:'street', 3:'mountain'}\n",
    "plt.pie(chart,\n",
    "        explode=(0.01, 0.01, 0.01, 0.01, 0.01, 0.01) , \n",
    "        labels=class_names,autopct='%1.1f%%')\n",
    "# plt.axis('equal')\n",
    "plt.title('Training Categorical Breakdown')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current working directory path + log dir\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "#returns a file path of the ending in the current time down to seconds\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_logdir, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 150, 150, 16)      448       \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 75, 75, 32)        4640      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 38, 38, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 19, 19, 64)        0         \n_________________________________________________________________\nflatten (Flatten)            (None, 23104)             0         \n_________________________________________________________________\ndense (Dense)                (None, 64)                1478720   \n_________________________________________________________________\ndropout (Dropout)            (None, 64)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_2 (Dense)              (None, 6)                 198       \n=================================================================\nTotal params: 1,504,582\nTrainable params: 1,504,582\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model_1 = Models.Sequential()\n",
    "\n",
    "model_1.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_1.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_1.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_1.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_1.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_1.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_1.add(Layers.Flatten())\n",
    "\n",
    "model_1.add(Layers.Dense(64,activation='relu'))\n",
    "model_1.add(Layers.Dropout(rate=0.5))\n",
    "model_1.add(Layers.Dense(32,activation='relu'))\n",
    "\n",
    "model_1.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_1.compile(optimizer=Optimizer.Adam(lr=0.01),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/50\n",
      "11227/11227 [==============================] - 5s 486us/sample - loss: 6.7253 - accuracy: 0.1704 - val_loss: 1.7926 - val_accuracy: 0.1589\n",
      "Epoch 2/50\n",
      "11227/11227 [==============================] - 4s 398us/sample - loss: 1.7917 - accuracy: 0.1746 - val_loss: 1.7939 - val_accuracy: 0.1589\n",
      "Epoch 3/50\n",
      "11227/11227 [==============================] - 5s 404us/sample - loss: 1.7912 - accuracy: 0.1747 - val_loss: 1.7958 - val_accuracy: 0.1589\n",
      "Epoch 4/50\n",
      "11227/11227 [==============================] - 4s 399us/sample - loss: 1.7927 - accuracy: 0.1741 - val_loss: 1.7940 - val_accuracy: 0.1585\n",
      "Epoch 5/50\n",
      "11227/11227 [==============================] - 5s 403us/sample - loss: 1.7918 - accuracy: 0.1737 - val_loss: 1.7923 - val_accuracy: 0.1799\n",
      "Epoch 6/50\n",
      "11227/11227 [==============================] - 4s 395us/sample - loss: 1.7920 - accuracy: 0.1740 - val_loss: 1.7934 - val_accuracy: 0.1799\n",
      "Epoch 7/50\n",
      "11227/11227 [==============================] - 4s 393us/sample - loss: 1.7917 - accuracy: 0.1739 - val_loss: 1.7930 - val_accuracy: 0.1653\n",
      "Epoch 8/50\n",
      "11227/11227 [==============================] - 4s 388us/sample - loss: 1.7917 - accuracy: 0.1705 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 9/50\n",
      "11227/11227 [==============================] - 4s 393us/sample - loss: 1.7918 - accuracy: 0.1768 - val_loss: 1.7930 - val_accuracy: 0.1799\n",
      "Epoch 10/50\n",
      "11227/11227 [==============================] - 4s 397us/sample - loss: 1.7914 - accuracy: 0.1808 - val_loss: 1.7926 - val_accuracy: 0.1799\n",
      "Epoch 11/50\n",
      "11227/11227 [==============================] - 4s 381us/sample - loss: 1.7913 - accuracy: 0.1716 - val_loss: 1.7933 - val_accuracy: 0.1653\n",
      "Epoch 12/50\n",
      "11227/11227 [==============================] - 4s 373us/sample - loss: 1.7957 - accuracy: 0.1759 - val_loss: 1.7922 - val_accuracy: 0.1589\n",
      "Epoch 13/50\n",
      "11227/11227 [==============================] - 4s 393us/sample - loss: 1.7916 - accuracy: 0.1705 - val_loss: 1.7947 - val_accuracy: 0.1589\n",
      "Epoch 14/50\n",
      "11227/11227 [==============================] - 4s 395us/sample - loss: 1.7916 - accuracy: 0.1756 - val_loss: 1.7937 - val_accuracy: 0.1799\n",
      "Epoch 15/50\n",
      "11227/11227 [==============================] - 4s 395us/sample - loss: 1.7916 - accuracy: 0.1683 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 16/50\n",
      "11227/11227 [==============================] - 4s 399us/sample - loss: 1.7920 - accuracy: 0.1701 - val_loss: 1.7923 - val_accuracy: 0.1653\n",
      "Epoch 17/50\n",
      "  192/11227 [..............................] - ETA: 4s - loss: 1.7961 - accuracy: 0.1250"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-2e80f7de3bf9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     callbacks=[tensorboard_cb])\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_1.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: saved_model\\3x1_1mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_1.save('saved_model\\\\3x1_1mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_12 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_14 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_15 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 19, 19, 64)        0         \n_________________________________________________________________\nflatten_3 (Flatten)          (None, 23104)             0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 64)                1478720   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 1,553,078\nTrainable params: 1,553,078\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2 = Models.Sequential()\n",
    "\n",
    "model_2.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_2.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_2.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_2.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_2.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_2.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_2.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_2.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_2.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_2.add(Layers.Flatten())\n",
    "\n",
    "model_2.add(Layers.Dense(64,activation='relu'))\n",
    "model_2.add(Layers.Dropout(rate=0.5))\n",
    "model_2.add(Layers.Dense(32,activation='relu'))\n",
    "model_2.add(Layers.Dropout(rate=0.5))\n",
    "model_2.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_2.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/50\n",
      "11227/11227 [==============================] - 8s 709us/sample - loss: 1.9935 - accuracy: 0.1766 - val_loss: 1.7917 - val_accuracy: 0.1799\n",
      "Epoch 2/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7922 - accuracy: 0.1789 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 3/50\n",
      "11227/11227 [==============================] - 7s 644us/sample - loss: 1.7914 - accuracy: 0.1789 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 4/50\n",
      "11227/11227 [==============================] - 7s 644us/sample - loss: 1.7912 - accuracy: 0.1788 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 5/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7911 - accuracy: 0.1788 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 6/50\n",
      "11227/11227 [==============================] - 7s 644us/sample - loss: 1.7910 - accuracy: 0.1788 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 7/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7909 - accuracy: 0.1788 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 8/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7908 - accuracy: 0.1788 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 9/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7907 - accuracy: 0.1789 - val_loss: 1.7916 - val_accuracy: 0.1799\n",
      "Epoch 10/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7907 - accuracy: 0.1788 - val_loss: 1.7917 - val_accuracy: 0.1799\n",
      "Epoch 11/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7910 - accuracy: 0.1788 - val_loss: 1.7917 - val_accuracy: 0.1799\n",
      "Epoch 12/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7907 - accuracy: 0.1788 - val_loss: 1.7917 - val_accuracy: 0.1799\n",
      "Epoch 13/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7906 - accuracy: 0.1788 - val_loss: 1.7917 - val_accuracy: 0.1799\n",
      "Epoch 14/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7906 - accuracy: 0.1788 - val_loss: 1.7918 - val_accuracy: 0.1799\n",
      "Epoch 15/50\n",
      "11227/11227 [==============================] - 7s 645us/sample - loss: 1.7906 - accuracy: 0.1788 - val_loss: 1.7918 - val_accuracy: 0.1799\n",
      "Epoch 16/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7906 - accuracy: 0.1788 - val_loss: 1.7918 - val_accuracy: 0.1799\n",
      "Epoch 17/50\n",
      "11227/11227 [==============================] - 7s 650us/sample - loss: 1.7906 - accuracy: 0.1788 - val_loss: 1.7918 - val_accuracy: 0.1799\n",
      "Epoch 18/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7918 - val_accuracy: 0.1799\n",
      "Epoch 19/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 20/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 21/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 22/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 23/50\n",
      "11227/11227 [==============================] - 7s 650us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 24/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7919 - val_accuracy: 0.1799\n",
      "Epoch 25/50\n",
      "11227/11227 [==============================] - 7s 651us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 26/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 27/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 28/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 29/50\n",
      "11227/11227 [==============================] - 7s 650us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 30/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 31/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 32/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7920 - val_accuracy: 0.1799\n",
      "Epoch 33/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 34/50\n",
      "11227/11227 [==============================] - 7s 650us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 35/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 36/50\n",
      "11227/11227 [==============================] - 7s 644us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 37/50\n",
      "11227/11227 [==============================] - 7s 651us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 38/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 39/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 40/50\n",
      "11227/11227 [==============================] - 7s 648us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 41/50\n",
      "11227/11227 [==============================] - 7s 650us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 42/50\n",
      "11227/11227 [==============================] - 7s 650us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 43/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 44/50\n",
      "11227/11227 [==============================] - 7s 645us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 45/50\n",
      "11227/11227 [==============================] - 7s 645us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 46/50\n",
      "11227/11227 [==============================] - 7s 646us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 47/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 48/50\n",
      "11227/11227 [==============================] - 7s 647us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 49/50\n",
      "11227/11227 [==============================] - 7s 649us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n",
      "Epoch 50/50\n",
      "11227/11227 [==============================] - 7s 645us/sample - loss: 1.7905 - accuracy: 0.1788 - val_loss: 1.7921 - val_accuracy: 0.1799\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15735de3f88>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_2.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=50, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\3x2_1mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_2.save('saved_model\\\\3x2_1mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_18 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nconv2d_20 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_21 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_22 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nconv2d_23 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_13 (MaxPooling (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_24 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_25 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nconv2d_26 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_14 (MaxPooling (None, 19, 19, 64)        0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 23104)             0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 64)                1478720   \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_13 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_14 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 1,601,574\nTrainable params: 1,601,574\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Models.Sequential()\n",
    "\n",
    "model_3.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_3.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_3.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_3.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_3.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_3.add(Layers.Flatten())\n",
    "\n",
    "model_3.add(Layers.Dense(64,activation='relu'))\n",
    "model_3.add(Layers.Dropout(rate=0.5))\n",
    "model_3.add(Layers.Dense(32,activation='relu'))\n",
    "model_3.add(Layers.Dropout(rate=0.5))\n",
    "model_3.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_3.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/100\n",
      "11227/11227 [==============================] - 11s 1ms/sample - loss: 1.6693 - accuracy: 0.2565 - val_loss: 1.3761 - val_accuracy: 0.5027\n",
      "Epoch 2/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 1.4949 - accuracy: 0.3400 - val_loss: 1.1979 - val_accuracy: 0.5718\n",
      "Epoch 3/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 1.3845 - accuracy: 0.4120 - val_loss: 1.1022 - val_accuracy: 0.6281\n",
      "Epoch 4/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 1.2916 - accuracy: 0.4575 - val_loss: 0.9787 - val_accuracy: 0.6687\n",
      "Epoch 5/100\n",
      "11227/11227 [==============================] - 11s 952us/sample - loss: 1.2108 - accuracy: 0.4856 - val_loss: 0.9328 - val_accuracy: 0.6619\n",
      "Epoch 6/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 1.1611 - accuracy: 0.5098 - val_loss: 0.9432 - val_accuracy: 0.6676\n",
      "Epoch 7/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 1.0892 - accuracy: 0.5403 - val_loss: 0.8838 - val_accuracy: 0.6915\n",
      "Epoch 8/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 1.0513 - accuracy: 0.5601 - val_loss: 0.8569 - val_accuracy: 0.7000\n",
      "Epoch 9/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 1.0159 - accuracy: 0.5672 - val_loss: 0.8148 - val_accuracy: 0.7282\n",
      "Epoch 10/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.9693 - accuracy: 0.5890 - val_loss: 0.7819 - val_accuracy: 0.7225\n",
      "Epoch 11/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.9347 - accuracy: 0.6029 - val_loss: 0.8370 - val_accuracy: 0.6915\n",
      "Epoch 12/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.8994 - accuracy: 0.6212 - val_loss: 0.7664 - val_accuracy: 0.7396\n",
      "Epoch 13/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.8821 - accuracy: 0.6249 - val_loss: 0.7283 - val_accuracy: 0.7595\n",
      "Epoch 14/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.8420 - accuracy: 0.6441 - val_loss: 0.7203 - val_accuracy: 0.7720\n",
      "Epoch 15/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.7984 - accuracy: 0.6615 - val_loss: 0.7274 - val_accuracy: 0.7414\n",
      "Epoch 16/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.7642 - accuracy: 0.6769 - val_loss: 0.6917 - val_accuracy: 0.7862\n",
      "Epoch 17/100\n",
      "11227/11227 [==============================] - 11s 952us/sample - loss: 0.7470 - accuracy: 0.6868 - val_loss: 0.6684 - val_accuracy: 0.7880\n",
      "Epoch 18/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.7218 - accuracy: 0.7034 - val_loss: 0.7676 - val_accuracy: 0.7549\n",
      "Epoch 19/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.6806 - accuracy: 0.7190 - val_loss: 0.6257 - val_accuracy: 0.8005\n",
      "Epoch 20/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.6745 - accuracy: 0.7284 - val_loss: 0.6202 - val_accuracy: 0.8019\n",
      "Epoch 21/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.6443 - accuracy: 0.7380 - val_loss: 0.6395 - val_accuracy: 0.7887\n",
      "Epoch 22/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.6202 - accuracy: 0.7471 - val_loss: 0.6124 - val_accuracy: 0.8023\n",
      "Epoch 23/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.6118 - accuracy: 0.7528 - val_loss: 0.6471 - val_accuracy: 0.8019\n",
      "Epoch 24/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.5865 - accuracy: 0.7596 - val_loss: 0.6935 - val_accuracy: 0.8030\n",
      "Epoch 25/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.5457 - accuracy: 0.7746 - val_loss: 0.6268 - val_accuracy: 0.8073\n",
      "Epoch 26/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.5251 - accuracy: 0.7833 - val_loss: 0.6460 - val_accuracy: 0.8023\n",
      "Epoch 27/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.5139 - accuracy: 0.7916 - val_loss: 0.7071 - val_accuracy: 0.7998\n",
      "Epoch 28/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.5077 - accuracy: 0.7957 - val_loss: 0.5803 - val_accuracy: 0.8190\n",
      "Epoch 29/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.4770 - accuracy: 0.8041 - val_loss: 0.7644 - val_accuracy: 0.8062\n",
      "Epoch 30/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.4824 - accuracy: 0.8032 - val_loss: 0.7142 - val_accuracy: 0.8069\n",
      "Epoch 31/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.4706 - accuracy: 0.8091 - val_loss: 0.7366 - val_accuracy: 0.7991\n",
      "Epoch 32/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.4534 - accuracy: 0.8140 - val_loss: 0.6713 - val_accuracy: 0.8151\n",
      "Epoch 33/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.4591 - accuracy: 0.8154 - val_loss: 0.6757 - val_accuracy: 0.8123\n",
      "Epoch 34/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.4528 - accuracy: 0.8177 - val_loss: 0.6673 - val_accuracy: 0.8001\n",
      "Epoch 35/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.4204 - accuracy: 0.8259 - val_loss: 0.6502 - val_accuracy: 0.8176\n",
      "Epoch 36/100\n",
      "11227/11227 [==============================] - 11s 951us/sample - loss: 0.4125 - accuracy: 0.8334 - val_loss: 0.7006 - val_accuracy: 0.8126\n",
      "Epoch 37/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.3896 - accuracy: 0.8346 - val_loss: 0.7550 - val_accuracy: 0.8222\n",
      "Epoch 38/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.4355 - accuracy: 0.8296 - val_loss: 0.6654 - val_accuracy: 0.8254\n",
      "Epoch 39/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.3842 - accuracy: 0.8437 - val_loss: 0.7194 - val_accuracy: 0.8094\n",
      "Epoch 40/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3797 - accuracy: 0.8419 - val_loss: 0.7698 - val_accuracy: 0.8155\n",
      "Epoch 41/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.3781 - accuracy: 0.8499 - val_loss: 0.8439 - val_accuracy: 0.8108\n",
      "Epoch 42/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.3787 - accuracy: 0.8471 - val_loss: 0.7297 - val_accuracy: 0.8155\n",
      "Epoch 43/100\n",
      "11227/11227 [==============================] - 11s 942us/sample - loss: 0.3716 - accuracy: 0.8498 - val_loss: 0.8034 - val_accuracy: 0.8187\n",
      "Epoch 44/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.3506 - accuracy: 0.8563 - val_loss: 0.8640 - val_accuracy: 0.8062\n",
      "Epoch 45/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.3518 - accuracy: 0.8537 - val_loss: 0.8886 - val_accuracy: 0.8247\n",
      "Epoch 46/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3595 - accuracy: 0.8541 - val_loss: 0.7524 - val_accuracy: 0.7898\n",
      "Epoch 47/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.3667 - accuracy: 0.8536 - val_loss: 0.7945 - val_accuracy: 0.8098\n",
      "Epoch 48/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3230 - accuracy: 0.8685 - val_loss: 0.8338 - val_accuracy: 0.8126\n",
      "Epoch 49/100\n",
      "11227/11227 [==============================] - 11s 944us/sample - loss: 0.3202 - accuracy: 0.8750 - val_loss: 1.0387 - val_accuracy: 0.8051\n",
      "Epoch 50/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3319 - accuracy: 0.8656 - val_loss: 0.9155 - val_accuracy: 0.8187\n",
      "Epoch 51/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.3447 - accuracy: 0.8624 - val_loss: 0.8676 - val_accuracy: 0.8140\n",
      "Epoch 52/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.3150 - accuracy: 0.8716 - val_loss: 0.9646 - val_accuracy: 0.7980\n",
      "Epoch 53/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.3202 - accuracy: 0.8716 - val_loss: 0.9105 - val_accuracy: 0.8108\n",
      "Epoch 54/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.3314 - accuracy: 0.8676 - val_loss: 0.8936 - val_accuracy: 0.8123\n",
      "Epoch 55/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3210 - accuracy: 0.8716 - val_loss: 0.9380 - val_accuracy: 0.8130\n",
      "Epoch 56/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3107 - accuracy: 0.8787 - val_loss: 1.0468 - val_accuracy: 0.8073\n",
      "Epoch 57/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.3018 - accuracy: 0.8749 - val_loss: 0.9539 - val_accuracy: 0.8098\n",
      "Epoch 58/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.3086 - accuracy: 0.8769 - val_loss: 1.0984 - val_accuracy: 0.7905\n",
      "Epoch 59/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.3035 - accuracy: 0.8799 - val_loss: 1.0126 - val_accuracy: 0.8137\n",
      "Epoch 60/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.3217 - accuracy: 0.8727 - val_loss: 0.9594 - val_accuracy: 0.8169\n",
      "Epoch 61/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.2964 - accuracy: 0.8841 - val_loss: 1.0005 - val_accuracy: 0.8172\n",
      "Epoch 62/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.3483 - accuracy: 0.8660 - val_loss: 0.8477 - val_accuracy: 0.8108\n",
      "Epoch 63/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.2942 - accuracy: 0.8843 - val_loss: 0.9072 - val_accuracy: 0.8204\n",
      "Epoch 64/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.3052 - accuracy: 0.8810 - val_loss: 0.9627 - val_accuracy: 0.8187\n",
      "Epoch 65/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2899 - accuracy: 0.8836 - val_loss: 1.0832 - val_accuracy: 0.8147\n",
      "Epoch 66/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.3052 - accuracy: 0.8826 - val_loss: 0.9576 - val_accuracy: 0.8123\n",
      "Epoch 67/100\n",
      "11227/11227 [==============================] - 11s 952us/sample - loss: 0.2854 - accuracy: 0.8865 - val_loss: 0.9826 - val_accuracy: 0.8197\n",
      "Epoch 68/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.2829 - accuracy: 0.8904 - val_loss: 1.0136 - val_accuracy: 0.8137\n",
      "Epoch 69/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.2834 - accuracy: 0.8860 - val_loss: 1.1160 - val_accuracy: 0.7955\n",
      "Epoch 70/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2834 - accuracy: 0.8883 - val_loss: 1.2178 - val_accuracy: 0.8112\n",
      "Epoch 71/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2904 - accuracy: 0.8859 - val_loss: 0.9794 - val_accuracy: 0.8201\n",
      "Epoch 72/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2861 - accuracy: 0.8855 - val_loss: 0.9459 - val_accuracy: 0.8187\n",
      "Epoch 73/100\n",
      "11227/11227 [==============================] - 11s 950us/sample - loss: 0.2933 - accuracy: 0.8873 - val_loss: 1.1341 - val_accuracy: 0.8126\n",
      "Epoch 74/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.3122 - accuracy: 0.8830 - val_loss: 1.0132 - val_accuracy: 0.8140\n",
      "Epoch 75/100\n",
      "11227/11227 [==============================] - 11s 944us/sample - loss: 0.2713 - accuracy: 0.8961 - val_loss: 1.1517 - val_accuracy: 0.8076\n",
      "Epoch 76/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.2717 - accuracy: 0.8915 - val_loss: 1.0883 - val_accuracy: 0.8115\n",
      "Epoch 77/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.2968 - accuracy: 0.8859 - val_loss: 1.1694 - val_accuracy: 0.8037\n",
      "Epoch 78/100\n",
      "11227/11227 [==============================] - 11s 951us/sample - loss: 0.2820 - accuracy: 0.8919 - val_loss: 1.1580 - val_accuracy: 0.8023\n",
      "Epoch 79/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.2706 - accuracy: 0.8938 - val_loss: 1.1118 - val_accuracy: 0.8147\n",
      "Epoch 80/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2662 - accuracy: 0.8948 - val_loss: 1.1993 - val_accuracy: 0.8237\n",
      "Epoch 81/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2528 - accuracy: 0.8998 - val_loss: 1.2835 - val_accuracy: 0.8140\n",
      "Epoch 82/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2742 - accuracy: 0.8967 - val_loss: 1.2923 - val_accuracy: 0.8169\n",
      "Epoch 83/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.2736 - accuracy: 0.8890 - val_loss: 1.2452 - val_accuracy: 0.8062\n",
      "Epoch 84/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2939 - accuracy: 0.8874 - val_loss: 0.9577 - val_accuracy: 0.8133\n",
      "Epoch 85/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2751 - accuracy: 0.8959 - val_loss: 1.2020 - val_accuracy: 0.8144\n",
      "Epoch 86/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.2726 - accuracy: 0.8950 - val_loss: 1.2119 - val_accuracy: 0.8194\n",
      "Epoch 87/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2684 - accuracy: 0.8978 - val_loss: 1.2708 - val_accuracy: 0.8094\n",
      "Epoch 88/100\n",
      "11227/11227 [==============================] - 11s 949us/sample - loss: 0.2679 - accuracy: 0.8949 - val_loss: 1.0290 - val_accuracy: 0.8076\n",
      "Epoch 89/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2662 - accuracy: 0.8992 - val_loss: 1.0821 - val_accuracy: 0.8026\n",
      "Epoch 90/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.2683 - accuracy: 0.8970 - val_loss: 1.0964 - val_accuracy: 0.8133\n",
      "Epoch 91/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.2459 - accuracy: 0.9029 - val_loss: 1.2066 - val_accuracy: 0.8119\n",
      "Epoch 92/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2750 - accuracy: 0.8971 - val_loss: 1.1097 - val_accuracy: 0.8169\n",
      "Epoch 93/100\n",
      "11227/11227 [==============================] - 11s 948us/sample - loss: 0.2585 - accuracy: 0.9059 - val_loss: 1.1927 - val_accuracy: 0.8165\n",
      "Epoch 94/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2548 - accuracy: 0.9029 - val_loss: 1.2421 - val_accuracy: 0.8076\n",
      "Epoch 95/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2659 - accuracy: 0.9012 - val_loss: 1.2406 - val_accuracy: 0.8190\n",
      "Epoch 96/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.2555 - accuracy: 0.9046 - val_loss: 1.1094 - val_accuracy: 0.8212\n",
      "Epoch 97/100\n",
      "11227/11227 [==============================] - 11s 946us/sample - loss: 0.2457 - accuracy: 0.9052 - val_loss: 1.1857 - val_accuracy: 0.8133\n",
      "Epoch 98/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2495 - accuracy: 0.9034 - val_loss: 1.3962 - val_accuracy: 0.8051\n",
      "Epoch 99/100\n",
      "11227/11227 [==============================] - 11s 947us/sample - loss: 0.2389 - accuracy: 0.9099 - val_loss: 1.2126 - val_accuracy: 0.8098\n",
      "Epoch 100/100\n",
      "11227/11227 [==============================] - 11s 945us/sample - loss: 0.2498 - accuracy: 0.9018 - val_loss: 1.3419 - val_accuracy: 0.8219\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15735f70d88>"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_3.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\3x3_1mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_3.save('saved_model\\\\3x3_1mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_27 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_28 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nmax_pooling2d_15 (MaxPooling (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_29 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_30 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_16 (MaxPooling (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_31 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_32 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_17 (MaxPooling (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_33 (Conv2D)           (None, 19, 19, 128)       73856     \n_________________________________________________________________\nconv2d_34 (Conv2D)           (None, 19, 19, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_18 (MaxPooling (None, 10, 10, 128)       0         \n_________________________________________________________________\nflatten_5 (Flatten)          (None, 12800)             0         \n_________________________________________________________________\ndense_15 (Dense)             (None, 128)               1638528   \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_16 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_17 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 32)                0         \n_________________________________________________________________\ndense_18 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 1,942,582\nTrainable params: 1,942,582\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4 = Models.Sequential()\n",
    "\n",
    "model_4.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_4.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_4.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_4.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_4.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_4.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "\n",
    "model_4.add(Layers.Flatten())\n",
    "model_4.add(Layers.Dense(128,activation='relu'))\n",
    "model_4.add(Layers.Dropout(rate=0.5))\n",
    "model_4.add(Layers.Dense(64,activation='relu'))\n",
    "model_4.add(Layers.Dropout(rate=0.5))\n",
    "model_4.add(Layers.Dense(32,activation='relu'))\n",
    "model_4.add(Layers.Dropout(rate=0.5))\n",
    "model_4.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_4.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/100\n",
      "11227/11227 [==============================] - 9s 817us/sample - loss: 1.8522 - accuracy: 0.2354 - val_loss: 1.7348 - val_accuracy: 0.3587\n",
      "Epoch 2/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 1.7148 - accuracy: 0.2861 - val_loss: 1.6198 - val_accuracy: 0.4282\n",
      "Epoch 3/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 1.6633 - accuracy: 0.3222 - val_loss: 1.6119 - val_accuracy: 0.4275\n",
      "Epoch 4/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 1.6257 - accuracy: 0.3293 - val_loss: 1.5415 - val_accuracy: 0.4407\n",
      "Epoch 5/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 1.5851 - accuracy: 0.3405 - val_loss: 1.4931 - val_accuracy: 0.4435\n",
      "Epoch 6/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.5379 - accuracy: 0.3611 - val_loss: 1.4313 - val_accuracy: 0.4418\n",
      "Epoch 7/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.4935 - accuracy: 0.3848 - val_loss: 1.3924 - val_accuracy: 0.4407\n",
      "Epoch 8/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 1.4585 - accuracy: 0.3966 - val_loss: 1.3526 - val_accuracy: 0.4678\n",
      "Epoch 9/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 1.4306 - accuracy: 0.4051 - val_loss: 1.4858 - val_accuracy: 0.4414\n",
      "Epoch 10/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.3897 - accuracy: 0.4262 - val_loss: 1.2691 - val_accuracy: 0.5126\n",
      "Epoch 11/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 1.3457 - accuracy: 0.4444 - val_loss: 1.2989 - val_accuracy: 0.5098\n",
      "Epoch 12/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 1.3343 - accuracy: 0.4522 - val_loss: 1.2262 - val_accuracy: 0.5305\n",
      "Epoch 13/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 1.2886 - accuracy: 0.4745 - val_loss: 1.1874 - val_accuracy: 0.5308\n",
      "Epoch 14/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.2414 - accuracy: 0.4883 - val_loss: 1.1663 - val_accuracy: 0.5436\n",
      "Epoch 15/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 1.2124 - accuracy: 0.4986 - val_loss: 1.1092 - val_accuracy: 0.5629\n",
      "Epoch 16/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 1.1883 - accuracy: 0.5059 - val_loss: 1.0628 - val_accuracy: 0.5732\n",
      "Epoch 17/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.1430 - accuracy: 0.5200 - val_loss: 1.0686 - val_accuracy: 0.5618\n",
      "Epoch 18/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.1121 - accuracy: 0.5301 - val_loss: 1.0203 - val_accuracy: 0.5771\n",
      "Epoch 19/100\n",
      "11227/11227 [==============================] - 8s 708us/sample - loss: 1.0751 - accuracy: 0.5537 - val_loss: 1.0272 - val_accuracy: 0.5611\n",
      "Epoch 20/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 1.0584 - accuracy: 0.5497 - val_loss: 0.9929 - val_accuracy: 0.5675\n",
      "Epoch 21/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 1.0073 - accuracy: 0.5662 - val_loss: 0.9649 - val_accuracy: 0.5625\n",
      "Epoch 22/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.9963 - accuracy: 0.5645 - val_loss: 0.9700 - val_accuracy: 0.5689\n",
      "Epoch 23/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.9682 - accuracy: 0.5774 - val_loss: 0.9410 - val_accuracy: 0.5696\n",
      "Epoch 24/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.9492 - accuracy: 0.5808 - val_loss: 0.9251 - val_accuracy: 0.5843\n",
      "Epoch 25/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.9300 - accuracy: 0.5838 - val_loss: 0.9288 - val_accuracy: 0.5729\n",
      "Epoch 26/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.9120 - accuracy: 0.5912 - val_loss: 0.9537 - val_accuracy: 0.5590\n",
      "Epoch 27/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.8744 - accuracy: 0.6054 - val_loss: 0.9239 - val_accuracy: 0.5800\n",
      "Epoch 28/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.8549 - accuracy: 0.6027 - val_loss: 0.9050 - val_accuracy: 0.5839\n",
      "Epoch 29/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.8406 - accuracy: 0.6095 - val_loss: 0.9336 - val_accuracy: 0.5821\n",
      "Epoch 30/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 0.8412 - accuracy: 0.6043 - val_loss: 0.9200 - val_accuracy: 0.5718\n",
      "Epoch 31/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.8196 - accuracy: 0.6102 - val_loss: 1.0473 - val_accuracy: 0.5718\n",
      "Epoch 32/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.7977 - accuracy: 0.6223 - val_loss: 0.9768 - val_accuracy: 0.5736\n",
      "Epoch 33/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.7898 - accuracy: 0.6185 - val_loss: 0.9230 - val_accuracy: 0.5750\n",
      "Epoch 34/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.7636 - accuracy: 0.6315 - val_loss: 0.9803 - val_accuracy: 0.5839\n",
      "Epoch 35/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.7613 - accuracy: 0.6267 - val_loss: 0.9582 - val_accuracy: 0.5878\n",
      "Epoch 36/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.7545 - accuracy: 0.6382 - val_loss: 0.9478 - val_accuracy: 0.5782\n",
      "Epoch 37/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.7223 - accuracy: 0.6410 - val_loss: 1.0146 - val_accuracy: 0.5885\n",
      "Epoch 38/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.7474 - accuracy: 0.6303 - val_loss: 0.9930 - val_accuracy: 0.5761\n",
      "Epoch 39/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.7205 - accuracy: 0.6401 - val_loss: 1.0426 - val_accuracy: 0.5793\n",
      "Epoch 40/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.7182 - accuracy: 0.6434 - val_loss: 1.0731 - val_accuracy: 0.5871\n",
      "Epoch 41/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.7115 - accuracy: 0.6467 - val_loss: 1.0458 - val_accuracy: 0.5689\n",
      "Epoch 42/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.7014 - accuracy: 0.6475 - val_loss: 1.0170 - val_accuracy: 0.5764\n",
      "Epoch 43/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.6990 - accuracy: 0.6471 - val_loss: 1.1232 - val_accuracy: 0.5910\n",
      "Epoch 44/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.6922 - accuracy: 0.6534 - val_loss: 1.1032 - val_accuracy: 0.5825\n",
      "Epoch 45/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.6910 - accuracy: 0.6500 - val_loss: 1.0078 - val_accuracy: 0.5850\n",
      "Epoch 46/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.6768 - accuracy: 0.6562 - val_loss: 1.0507 - val_accuracy: 0.5821\n",
      "Epoch 47/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.6772 - accuracy: 0.6642 - val_loss: 1.1580 - val_accuracy: 0.5889\n",
      "Epoch 48/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 0.6583 - accuracy: 0.6612 - val_loss: 1.1387 - val_accuracy: 0.5810\n",
      "Epoch 49/100\n",
      "11227/11227 [==============================] - 8s 712us/sample - loss: 0.6829 - accuracy: 0.6595 - val_loss: 1.0602 - val_accuracy: 0.5800\n",
      "Epoch 50/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.6679 - accuracy: 0.6575 - val_loss: 1.0891 - val_accuracy: 0.5486\n",
      "Epoch 51/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.6708 - accuracy: 0.6621 - val_loss: 1.1421 - val_accuracy: 0.5771\n",
      "Epoch 52/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.6466 - accuracy: 0.6647 - val_loss: 1.1783 - val_accuracy: 0.5736\n",
      "Epoch 53/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.6566 - accuracy: 0.6594 - val_loss: 1.1735 - val_accuracy: 0.5764\n",
      "Epoch 54/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.6629 - accuracy: 0.6634 - val_loss: 1.1476 - val_accuracy: 0.5885\n",
      "Epoch 55/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.6535 - accuracy: 0.6557 - val_loss: 1.2455 - val_accuracy: 0.5821\n",
      "Epoch 56/100\n",
      "11227/11227 [==============================] - 8s 711us/sample - loss: 0.6321 - accuracy: 0.6715 - val_loss: 1.1492 - val_accuracy: 0.5864\n",
      "Epoch 57/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.6736 - accuracy: 0.6595 - val_loss: 1.1749 - val_accuracy: 0.5782\n",
      "Epoch 58/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.6276 - accuracy: 0.6752 - val_loss: 1.2571 - val_accuracy: 0.5718\n",
      "Epoch 59/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.6370 - accuracy: 0.6885 - val_loss: 1.1110 - val_accuracy: 0.6302\n",
      "Epoch 60/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.6095 - accuracy: 0.7070 - val_loss: 1.1100 - val_accuracy: 0.6509\n",
      "Epoch 61/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.6152 - accuracy: 0.7080 - val_loss: 1.2747 - val_accuracy: 0.6534\n",
      "Epoch 62/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.5921 - accuracy: 0.7269 - val_loss: 1.2067 - val_accuracy: 0.6601\n",
      "Epoch 63/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.5895 - accuracy: 0.7355 - val_loss: 1.0693 - val_accuracy: 0.6555\n",
      "Epoch 64/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.5646 - accuracy: 0.7474 - val_loss: 1.1977 - val_accuracy: 0.6448\n",
      "Epoch 65/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.5624 - accuracy: 0.7455 - val_loss: 1.1793 - val_accuracy: 0.6655\n",
      "Epoch 66/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.5256 - accuracy: 0.7612 - val_loss: 1.2215 - val_accuracy: 0.7339\n",
      "Epoch 67/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.5283 - accuracy: 0.7628 - val_loss: 1.1086 - val_accuracy: 0.7453\n",
      "Epoch 68/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.5233 - accuracy: 0.7656 - val_loss: 1.1602 - val_accuracy: 0.7246\n",
      "Epoch 69/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.4916 - accuracy: 0.7738 - val_loss: 1.0341 - val_accuracy: 0.7264\n",
      "Epoch 70/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.4943 - accuracy: 0.7678 - val_loss: 1.3029 - val_accuracy: 0.7186\n",
      "Epoch 71/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.4883 - accuracy: 0.7736 - val_loss: 1.2850 - val_accuracy: 0.7453\n",
      "Epoch 72/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.4914 - accuracy: 0.7776 - val_loss: 1.1854 - val_accuracy: 0.7488\n",
      "Epoch 73/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.4768 - accuracy: 0.7813 - val_loss: 1.0823 - val_accuracy: 0.7367\n",
      "Epoch 74/100\n",
      "11227/11227 [==============================] - 8s 712us/sample - loss: 0.4609 - accuracy: 0.7849 - val_loss: 1.1097 - val_accuracy: 0.7214\n",
      "Epoch 75/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.4490 - accuracy: 0.7870 - val_loss: 1.2448 - val_accuracy: 0.7453\n",
      "Epoch 76/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.4502 - accuracy: 0.7885 - val_loss: 1.1926 - val_accuracy: 0.7449\n",
      "Epoch 77/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.4359 - accuracy: 0.7882 - val_loss: 1.2935 - val_accuracy: 0.7289\n",
      "Epoch 78/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.4382 - accuracy: 0.7932 - val_loss: 1.2059 - val_accuracy: 0.7478\n",
      "Epoch 79/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.4261 - accuracy: 0.7918 - val_loss: 1.3482 - val_accuracy: 0.7421\n",
      "Epoch 80/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.4289 - accuracy: 0.7942 - val_loss: 1.2458 - val_accuracy: 0.7506\n",
      "Epoch 81/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.4158 - accuracy: 0.8015 - val_loss: 1.1541 - val_accuracy: 0.7513\n",
      "Epoch 82/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.4263 - accuracy: 0.7984 - val_loss: 1.3302 - val_accuracy: 0.7339\n",
      "Epoch 83/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.3929 - accuracy: 0.8054 - val_loss: 1.2327 - val_accuracy: 0.7542\n",
      "Epoch 84/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.3984 - accuracy: 0.8024 - val_loss: 1.2355 - val_accuracy: 0.7396\n",
      "Epoch 85/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.4163 - accuracy: 0.8064 - val_loss: 1.2336 - val_accuracy: 0.7496\n",
      "Epoch 86/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.3802 - accuracy: 0.8130 - val_loss: 1.2223 - val_accuracy: 0.7257\n",
      "Epoch 87/100\n",
      "11227/11227 [==============================] - 8s 713us/sample - loss: 0.3866 - accuracy: 0.8113 - val_loss: 1.1838 - val_accuracy: 0.7531\n",
      "Epoch 88/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.3857 - accuracy: 0.8097 - val_loss: 1.2583 - val_accuracy: 0.7360\n",
      "Epoch 89/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.3765 - accuracy: 0.8178 - val_loss: 1.2616 - val_accuracy: 0.7506\n",
      "Epoch 90/100\n",
      "11227/11227 [==============================] - 8s 719us/sample - loss: 0.3730 - accuracy: 0.8126 - val_loss: 1.2936 - val_accuracy: 0.7506\n",
      "Epoch 91/100\n",
      "11227/11227 [==============================] - 8s 718us/sample - loss: 0.3719 - accuracy: 0.8111 - val_loss: 1.2381 - val_accuracy: 0.7478\n",
      "Epoch 92/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.3704 - accuracy: 0.8116 - val_loss: 1.4107 - val_accuracy: 0.7471\n",
      "Epoch 93/100\n",
      "11227/11227 [==============================] - 8s 716us/sample - loss: 0.3706 - accuracy: 0.8147 - val_loss: 1.4314 - val_accuracy: 0.7485\n",
      "Epoch 94/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.3764 - accuracy: 0.8082 - val_loss: 1.2225 - val_accuracy: 0.7374\n",
      "Epoch 95/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.3674 - accuracy: 0.8093 - val_loss: 1.3940 - val_accuracy: 0.7353\n",
      "Epoch 96/100\n",
      "11227/11227 [==============================] - 8s 714us/sample - loss: 0.3642 - accuracy: 0.8170 - val_loss: 1.1626 - val_accuracy: 0.7606\n",
      "Epoch 97/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.3702 - accuracy: 0.8113 - val_loss: 1.1681 - val_accuracy: 0.7382\n",
      "Epoch 98/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.3600 - accuracy: 0.8136 - val_loss: 1.2723 - val_accuracy: 0.7278\n",
      "Epoch 99/100\n",
      "11227/11227 [==============================] - 8s 715us/sample - loss: 0.3561 - accuracy: 0.8149 - val_loss: 1.2398 - val_accuracy: 0.7435\n",
      "Epoch 100/100\n",
      "11227/11227 [==============================] - 8s 717us/sample - loss: 0.3527 - accuracy: 0.8173 - val_loss: 1.2322 - val_accuracy: 0.7492\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x157365d4808>"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_4.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\4x2_2mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_4.save('saved_model\\\\4x2_2mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_35 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_36 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nconv2d_37 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nmax_pooling2d_19 (MaxPooling (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_38 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_39 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nconv2d_40 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_20 (MaxPooling (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_41 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_42 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nconv2d_43 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_21 (MaxPooling (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_44 (Conv2D)           (None, 19, 19, 128)       73856     \n_________________________________________________________________\nconv2d_45 (Conv2D)           (None, 19, 19, 128)       147584    \n_________________________________________________________________\nconv2d_46 (Conv2D)           (None, 19, 19, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_22 (MaxPooling (None, 10, 10, 128)       0         \n_________________________________________________________________\nflatten_6 (Flatten)          (None, 12800)             0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 128)               1638528   \n_________________________________________________________________\ndropout_11 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_12 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_21 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_13 (Dropout)         (None, 32)                0         \n_________________________________________________________________\ndense_22 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 2,138,662\nTrainable params: 2,138,662\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5 = Models.Sequential()\n",
    "\n",
    "model_5.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_5.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_5.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_5.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_5.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "\n",
    "model_5.add(Layers.Flatten())\n",
    "model_5.add(Layers.Dense(128,activation='relu'))\n",
    "model_5.add(Layers.Dropout(rate=0.5))\n",
    "model_5.add(Layers.Dense(64,activation='relu'))\n",
    "model_5.add(Layers.Dropout(rate=0.5))\n",
    "model_5.add(Layers.Dense(32,activation='relu'))\n",
    "model_5.add(Layers.Dropout(rate=0.5))\n",
    "model_5.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_5.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/100\n",
      "11227/11227 [==============================] - 13s 1ms/sample - loss: 1.7094 - accuracy: 0.2556 - val_loss: 1.4314 - val_accuracy: 0.5012\n",
      "Epoch 2/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.5505 - accuracy: 0.3576 - val_loss: 1.2514 - val_accuracy: 0.5322\n",
      "Epoch 3/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.4141 - accuracy: 0.4236 - val_loss: 1.1561 - val_accuracy: 0.5707\n",
      "Epoch 4/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2929 - accuracy: 0.4884 - val_loss: 0.9778 - val_accuracy: 0.6356\n",
      "Epoch 5/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2114 - accuracy: 0.5307 - val_loss: 0.9390 - val_accuracy: 0.6131\n",
      "Epoch 6/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.1395 - accuracy: 0.5584 - val_loss: 0.8576 - val_accuracy: 0.6961\n",
      "Epoch 7/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0811 - accuracy: 0.5856 - val_loss: 0.8279 - val_accuracy: 0.7275\n",
      "Epoch 8/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0170 - accuracy: 0.6211 - val_loss: 0.7665 - val_accuracy: 0.7734\n",
      "Epoch 9/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9734 - accuracy: 0.6421 - val_loss: 0.7081 - val_accuracy: 0.7770\n",
      "Epoch 10/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9209 - accuracy: 0.6633 - val_loss: 0.6467 - val_accuracy: 0.8051\n",
      "Epoch 11/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8630 - accuracy: 0.6943 - val_loss: 0.6370 - val_accuracy: 0.7855\n",
      "Epoch 12/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8192 - accuracy: 0.7116 - val_loss: 0.6603 - val_accuracy: 0.7834\n",
      "Epoch 13/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.7689 - accuracy: 0.7397 - val_loss: 0.5806 - val_accuracy: 0.8101\n",
      "Epoch 14/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.7238 - accuracy: 0.7620 - val_loss: 0.5542 - val_accuracy: 0.8265\n",
      "Epoch 15/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.6926 - accuracy: 0.7723 - val_loss: 0.5748 - val_accuracy: 0.8137\n",
      "Epoch 16/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.6597 - accuracy: 0.7827 - val_loss: 0.5515 - val_accuracy: 0.8183\n",
      "Epoch 17/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.6134 - accuracy: 0.8027 - val_loss: 0.6743 - val_accuracy: 0.7813\n",
      "Epoch 18/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.6128 - accuracy: 0.8053 - val_loss: 0.5130 - val_accuracy: 0.8457\n",
      "Epoch 19/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.5800 - accuracy: 0.8200 - val_loss: 0.5113 - val_accuracy: 0.8336\n",
      "Epoch 20/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.5551 - accuracy: 0.8236 - val_loss: 0.5392 - val_accuracy: 0.8254\n",
      "Epoch 21/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.5213 - accuracy: 0.8326 - val_loss: 0.5698 - val_accuracy: 0.8429\n",
      "Epoch 22/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.4910 - accuracy: 0.8440 - val_loss: 0.5278 - val_accuracy: 0.8440\n",
      "Epoch 23/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.4614 - accuracy: 0.8513 - val_loss: 0.5329 - val_accuracy: 0.8358\n",
      "Epoch 24/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.4558 - accuracy: 0.8545 - val_loss: 0.5718 - val_accuracy: 0.8237\n",
      "Epoch 25/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.4340 - accuracy: 0.8614 - val_loss: 0.5562 - val_accuracy: 0.8497\n",
      "Epoch 26/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.4191 - accuracy: 0.8694 - val_loss: 0.5458 - val_accuracy: 0.8564\n",
      "Epoch 27/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3879 - accuracy: 0.8749 - val_loss: 0.5757 - val_accuracy: 0.8400\n",
      "Epoch 28/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3787 - accuracy: 0.8784 - val_loss: 0.6183 - val_accuracy: 0.8532\n",
      "Epoch 29/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3615 - accuracy: 0.8857 - val_loss: 0.5819 - val_accuracy: 0.8486\n",
      "Epoch 30/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3546 - accuracy: 0.8863 - val_loss: 0.6302 - val_accuracy: 0.8482\n",
      "Epoch 31/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3170 - accuracy: 0.8986 - val_loss: 0.6438 - val_accuracy: 0.8400\n",
      "Epoch 32/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3120 - accuracy: 0.8983 - val_loss: 0.6355 - val_accuracy: 0.8504\n",
      "Epoch 33/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3063 - accuracy: 0.9038 - val_loss: 0.6540 - val_accuracy: 0.8390\n",
      "Epoch 34/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.3029 - accuracy: 0.9056 - val_loss: 0.7050 - val_accuracy: 0.8561\n",
      "Epoch 35/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2910 - accuracy: 0.9115 - val_loss: 0.6665 - val_accuracy: 0.8525\n",
      "Epoch 36/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2596 - accuracy: 0.9168 - val_loss: 0.7389 - val_accuracy: 0.8518\n",
      "Epoch 37/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2734 - accuracy: 0.9128 - val_loss: 0.7173 - val_accuracy: 0.8404\n",
      "Epoch 38/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2628 - accuracy: 0.9197 - val_loss: 0.7210 - val_accuracy: 0.8415\n",
      "Epoch 39/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2440 - accuracy: 0.9240 - val_loss: 0.7205 - val_accuracy: 0.8532\n",
      "Epoch 40/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2176 - accuracy: 0.9298 - val_loss: 0.7959 - val_accuracy: 0.8554\n",
      "Epoch 41/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2308 - accuracy: 0.9276 - val_loss: 0.8768 - val_accuracy: 0.8436\n",
      "Epoch 42/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2479 - accuracy: 0.9236 - val_loss: 0.7772 - val_accuracy: 0.8525\n",
      "Epoch 43/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2229 - accuracy: 0.9331 - val_loss: 0.8202 - val_accuracy: 0.8429\n",
      "Epoch 44/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1862 - accuracy: 0.9401 - val_loss: 0.8175 - val_accuracy: 0.8529\n",
      "Epoch 45/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2449 - accuracy: 0.9245 - val_loss: 0.9044 - val_accuracy: 0.7627\n",
      "Epoch 46/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2082 - accuracy: 0.9344 - val_loss: 0.7878 - val_accuracy: 0.8465\n",
      "Epoch 47/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1857 - accuracy: 0.9426 - val_loss: 0.9466 - val_accuracy: 0.8429\n",
      "Epoch 48/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1963 - accuracy: 0.9420 - val_loss: 0.7254 - val_accuracy: 0.8514\n",
      "Epoch 49/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1983 - accuracy: 0.9396 - val_loss: 0.8913 - val_accuracy: 0.8482\n",
      "Epoch 50/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1546 - accuracy: 0.9488 - val_loss: 1.0046 - val_accuracy: 0.8472\n",
      "Epoch 51/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.2039 - accuracy: 0.9394 - val_loss: 0.8628 - val_accuracy: 0.8390\n",
      "Epoch 52/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1773 - accuracy: 0.9466 - val_loss: 1.0243 - val_accuracy: 0.8497\n",
      "Epoch 53/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1681 - accuracy: 0.9491 - val_loss: 1.0220 - val_accuracy: 0.8479\n",
      "Epoch 54/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1443 - accuracy: 0.9576 - val_loss: 1.1840 - val_accuracy: 0.8518\n",
      "Epoch 55/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1666 - accuracy: 0.9520 - val_loss: 0.7872 - val_accuracy: 0.8393\n",
      "Epoch 56/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1707 - accuracy: 0.9535 - val_loss: 1.0686 - val_accuracy: 0.8432\n",
      "Epoch 57/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1437 - accuracy: 0.9570 - val_loss: 1.2089 - val_accuracy: 0.8354\n",
      "Epoch 58/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1361 - accuracy: 0.9585 - val_loss: 1.1746 - val_accuracy: 0.8333\n",
      "Epoch 59/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1557 - accuracy: 0.9546 - val_loss: 0.9534 - val_accuracy: 0.8368\n",
      "Epoch 60/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1508 - accuracy: 0.9534 - val_loss: 1.0666 - val_accuracy: 0.8497\n",
      "Epoch 61/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1326 - accuracy: 0.9584 - val_loss: 0.9655 - val_accuracy: 0.8489\n",
      "Epoch 62/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1399 - accuracy: 0.9601 - val_loss: 1.0975 - val_accuracy: 0.8564\n",
      "Epoch 63/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1645 - accuracy: 0.9573 - val_loss: 1.1797 - val_accuracy: 0.8261\n",
      "Epoch 64/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1203 - accuracy: 0.9646 - val_loss: 1.1006 - val_accuracy: 0.8482\n",
      "Epoch 65/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1059 - accuracy: 0.9677 - val_loss: 1.1632 - val_accuracy: 0.8497\n",
      "Epoch 66/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1409 - accuracy: 0.9566 - val_loss: 1.1089 - val_accuracy: 0.8308\n",
      "Epoch 67/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1367 - accuracy: 0.9577 - val_loss: 1.1111 - val_accuracy: 0.8536\n",
      "Epoch 68/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1039 - accuracy: 0.9685 - val_loss: 1.3465 - val_accuracy: 0.8454\n",
      "Epoch 69/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1404 - accuracy: 0.9613 - val_loss: 1.0335 - val_accuracy: 0.8304\n",
      "Epoch 70/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1053 - accuracy: 0.9684 - val_loss: 1.0807 - val_accuracy: 0.8479\n",
      "Epoch 71/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1272 - accuracy: 0.9629 - val_loss: 0.8648 - val_accuracy: 0.8489\n",
      "Epoch 72/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1339 - accuracy: 0.9613 - val_loss: 1.0527 - val_accuracy: 0.8400\n",
      "Epoch 73/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1298 - accuracy: 0.9629 - val_loss: 1.1887 - val_accuracy: 0.8422\n",
      "Epoch 74/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1327 - accuracy: 0.9606 - val_loss: 1.2133 - val_accuracy: 0.8386\n",
      "Epoch 75/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1010 - accuracy: 0.9703 - val_loss: 1.2746 - val_accuracy: 0.8372\n",
      "Epoch 76/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1207 - accuracy: 0.9663 - val_loss: 1.1527 - val_accuracy: 0.8347\n",
      "Epoch 77/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1108 - accuracy: 0.9692 - val_loss: 0.9913 - val_accuracy: 0.8347\n",
      "Epoch 78/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1185 - accuracy: 0.9678 - val_loss: 1.4064 - val_accuracy: 0.8343\n",
      "Epoch 79/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1032 - accuracy: 0.9701 - val_loss: 1.2909 - val_accuracy: 0.8525\n",
      "Epoch 80/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1089 - accuracy: 0.9707 - val_loss: 1.3426 - val_accuracy: 0.8454\n",
      "Epoch 81/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0921 - accuracy: 0.9719 - val_loss: 1.5220 - val_accuracy: 0.8311\n",
      "Epoch 82/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1044 - accuracy: 0.9720 - val_loss: 1.3632 - val_accuracy: 0.8479\n",
      "Epoch 83/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1147 - accuracy: 0.9683 - val_loss: 1.0522 - val_accuracy: 0.8472\n",
      "Epoch 84/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0899 - accuracy: 0.9752 - val_loss: 1.2884 - val_accuracy: 0.8475\n",
      "Epoch 85/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0856 - accuracy: 0.9746 - val_loss: 1.4719 - val_accuracy: 0.8440\n",
      "Epoch 86/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0947 - accuracy: 0.9723 - val_loss: 1.3459 - val_accuracy: 0.8397\n",
      "Epoch 87/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1593 - accuracy: 0.9604 - val_loss: 1.1489 - val_accuracy: 0.8436\n",
      "Epoch 88/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1087 - accuracy: 0.9693 - val_loss: 1.3201 - val_accuracy: 0.8383\n",
      "Epoch 89/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1033 - accuracy: 0.9709 - val_loss: 1.3700 - val_accuracy: 0.8379\n",
      "Epoch 90/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0922 - accuracy: 0.9728 - val_loss: 1.2438 - val_accuracy: 0.8436\n",
      "Epoch 91/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0849 - accuracy: 0.9760 - val_loss: 1.3338 - val_accuracy: 0.8457\n",
      "Epoch 92/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0805 - accuracy: 0.9768 - val_loss: 1.2130 - val_accuracy: 0.8436\n",
      "Epoch 93/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1066 - accuracy: 0.9717 - val_loss: 1.3073 - val_accuracy: 0.8343\n",
      "Epoch 94/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0847 - accuracy: 0.9768 - val_loss: 1.4070 - val_accuracy: 0.8383\n",
      "Epoch 95/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1050 - accuracy: 0.9710 - val_loss: 1.1759 - val_accuracy: 0.8383\n",
      "Epoch 96/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0652 - accuracy: 0.9803 - val_loss: 1.6540 - val_accuracy: 0.8532\n",
      "Epoch 97/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0865 - accuracy: 0.9764 - val_loss: 1.1768 - val_accuracy: 0.8297\n",
      "Epoch 98/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.0794 - accuracy: 0.9772 - val_loss: 1.2231 - val_accuracy: 0.8486\n",
      "Epoch 99/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1076 - accuracy: 0.9727 - val_loss: 1.3702 - val_accuracy: 0.8529\n",
      "Epoch 100/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.1015 - accuracy: 0.9735 - val_loss: 1.1862 - val_accuracy: 0.8454\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x157c7e7af48>"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_5.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\4x3_2mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_5.save('saved_model\\\\4x3_2mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_47 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_48 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nmax_pooling2d_23 (MaxPooling (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_49 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_50 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_24 (MaxPooling (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_51 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_52 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_25 (MaxPooling (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_53 (Conv2D)           (None, 19, 19, 128)       73856     \n_________________________________________________________________\nconv2d_54 (Conv2D)           (None, 19, 19, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_26 (MaxPooling (None, 10, 10, 128)       0         \n_________________________________________________________________\nconv2d_55 (Conv2D)           (None, 10, 10, 256)       295168    \n_________________________________________________________________\nconv2d_56 (Conv2D)           (None, 10, 10, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_27 (MaxPooling (None, 5, 5, 256)         0         \n_________________________________________________________________\nflatten_7 (Flatten)          (None, 6400)              0         \n_________________________________________________________________\ndense_23 (Dense)             (None, 256)               1638656   \n_________________________________________________________________\ndropout_14 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_24 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndropout_15 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_25 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_16 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_26 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_17 (Dropout)         (None, 32)                0         \n_________________________________________________________________\ndense_27 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 2,860,854\nTrainable params: 2,860,854\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_6 = Models.Sequential()\n",
    "\n",
    "model_6.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_6.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_6.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_6.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_6.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_6.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_6.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_6.add(Layers.Flatten())\n",
    "model_6.add(Layers.Dense(256,activation='relu'))\n",
    "model_6.add(Layers.Dropout(rate=0.5))\n",
    "model_6.add(Layers.Dense(128,activation='relu'))\n",
    "model_6.add(Layers.Dropout(rate=0.5))\n",
    "model_6.add(Layers.Dense(64,activation='relu'))\n",
    "model_6.add(Layers.Dropout(rate=0.5))\n",
    "model_6.add(Layers.Dense(32,activation='relu'))\n",
    "model_6.add(Layers.Dropout(rate=0.5))\n",
    "model_6.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_6.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/100\n",
      "11227/11227 [==============================] - 10s 911us/sample - loss: 1.8277 - accuracy: 0.1746 - val_loss: 1.7533 - val_accuracy: 0.3274\n",
      "Epoch 2/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 1.7153 - accuracy: 0.2429 - val_loss: 1.5242 - val_accuracy: 0.4507\n",
      "Epoch 3/100\n",
      "11227/11227 [==============================] - 9s 786us/sample - loss: 1.6016 - accuracy: 0.3180 - val_loss: 1.3732 - val_accuracy: 0.4913\n",
      "Epoch 4/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 1.4883 - accuracy: 0.3805 - val_loss: 1.2287 - val_accuracy: 0.5358\n",
      "Epoch 5/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 1.3894 - accuracy: 0.4364 - val_loss: 1.1196 - val_accuracy: 0.5525\n",
      "Epoch 6/100\n",
      "11227/11227 [==============================] - 9s 786us/sample - loss: 1.2990 - accuracy: 0.4843 - val_loss: 1.0401 - val_accuracy: 0.6067\n",
      "Epoch 7/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 1.2344 - accuracy: 0.5195 - val_loss: 1.0285 - val_accuracy: 0.6473\n",
      "Epoch 8/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 1.1600 - accuracy: 0.5469 - val_loss: 0.9428 - val_accuracy: 0.6840\n",
      "Epoch 9/100\n",
      "11227/11227 [==============================] - 9s 786us/sample - loss: 1.1153 - accuracy: 0.5791 - val_loss: 0.9510 - val_accuracy: 0.6797\n",
      "Epoch 10/100\n",
      "11227/11227 [==============================] - 9s 786us/sample - loss: 1.0576 - accuracy: 0.5983 - val_loss: 0.8660 - val_accuracy: 0.7203\n",
      "Epoch 11/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 1.0122 - accuracy: 0.6120 - val_loss: 0.8436 - val_accuracy: 0.7086\n",
      "Epoch 12/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.9560 - accuracy: 0.6400 - val_loss: 0.7805 - val_accuracy: 0.7221\n",
      "Epoch 13/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.9414 - accuracy: 0.6357 - val_loss: 0.7996 - val_accuracy: 0.6901\n",
      "Epoch 14/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 0.8969 - accuracy: 0.6545 - val_loss: 0.7722 - val_accuracy: 0.7075\n",
      "Epoch 15/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.8639 - accuracy: 0.6659 - val_loss: 0.7404 - val_accuracy: 0.7057\n",
      "Epoch 16/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.8390 - accuracy: 0.6759 - val_loss: 0.7252 - val_accuracy: 0.7250\n",
      "Epoch 17/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.8050 - accuracy: 0.6817 - val_loss: 0.7209 - val_accuracy: 0.7054\n",
      "Epoch 18/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.7895 - accuracy: 0.6914 - val_loss: 0.7045 - val_accuracy: 0.7089\n",
      "Epoch 19/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.7406 - accuracy: 0.7064 - val_loss: 0.7116 - val_accuracy: 0.6961\n",
      "Epoch 20/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.7152 - accuracy: 0.7168 - val_loss: 0.7453 - val_accuracy: 0.7029\n",
      "Epoch 21/100\n",
      "11227/11227 [==============================] - 9s 780us/sample - loss: 0.7250 - accuracy: 0.7143 - val_loss: 0.6785 - val_accuracy: 0.7271\n",
      "Epoch 22/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.6824 - accuracy: 0.7298 - val_loss: 0.6776 - val_accuracy: 0.7243\n",
      "Epoch 23/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.6691 - accuracy: 0.7349 - val_loss: 0.6733 - val_accuracy: 0.7360\n",
      "Epoch 24/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.6429 - accuracy: 0.7406 - val_loss: 0.7049 - val_accuracy: 0.7018\n",
      "Epoch 25/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.6071 - accuracy: 0.7540 - val_loss: 0.6941 - val_accuracy: 0.7613\n",
      "Epoch 26/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.5665 - accuracy: 0.7714 - val_loss: 0.7682 - val_accuracy: 0.7756\n",
      "Epoch 27/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.5566 - accuracy: 0.7757 - val_loss: 0.7256 - val_accuracy: 0.7670\n",
      "Epoch 28/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.5464 - accuracy: 0.7780 - val_loss: 0.6828 - val_accuracy: 0.8058\n",
      "Epoch 29/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.5495 - accuracy: 0.7803 - val_loss: 0.7003 - val_accuracy: 0.8137\n",
      "Epoch 30/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 0.4981 - accuracy: 0.8019 - val_loss: 0.7741 - val_accuracy: 0.8172\n",
      "Epoch 31/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.4968 - accuracy: 0.8045 - val_loss: 0.7988 - val_accuracy: 0.8169\n",
      "Epoch 32/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.4677 - accuracy: 0.8238 - val_loss: 0.8368 - val_accuracy: 0.8162\n",
      "Epoch 33/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.4804 - accuracy: 0.8252 - val_loss: 0.8512 - val_accuracy: 0.8055\n",
      "Epoch 34/100\n",
      "11227/11227 [==============================] - 9s 786us/sample - loss: 0.4155 - accuracy: 0.8490 - val_loss: 0.7550 - val_accuracy: 0.8041\n",
      "Epoch 35/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.4120 - accuracy: 0.8562 - val_loss: 0.8169 - val_accuracy: 0.8340\n",
      "Epoch 36/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.3860 - accuracy: 0.8698 - val_loss: 0.9617 - val_accuracy: 0.8283\n",
      "Epoch 37/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.3573 - accuracy: 0.8863 - val_loss: 0.8091 - val_accuracy: 0.8279\n",
      "Epoch 38/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.3757 - accuracy: 0.8814 - val_loss: 0.7285 - val_accuracy: 0.8454\n",
      "Epoch 39/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.3160 - accuracy: 0.8984 - val_loss: 0.8747 - val_accuracy: 0.8514\n",
      "Epoch 40/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.3074 - accuracy: 0.9009 - val_loss: 0.8517 - val_accuracy: 0.8215\n",
      "Epoch 41/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.2901 - accuracy: 0.9139 - val_loss: 0.7147 - val_accuracy: 0.8429\n",
      "Epoch 42/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.3402 - accuracy: 0.9016 - val_loss: 0.8059 - val_accuracy: 0.8354\n",
      "Epoch 43/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.2678 - accuracy: 0.9198 - val_loss: 0.7506 - val_accuracy: 0.8468\n",
      "Epoch 44/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.2825 - accuracy: 0.9151 - val_loss: 0.7845 - val_accuracy: 0.8454\n",
      "Epoch 45/100\n",
      "11227/11227 [==============================] - 9s 775us/sample - loss: 0.2248 - accuracy: 0.9341 - val_loss: 0.9236 - val_accuracy: 0.8333\n",
      "Epoch 46/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.2396 - accuracy: 0.9298 - val_loss: 0.9120 - val_accuracy: 0.8343\n",
      "Epoch 47/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.2639 - accuracy: 0.9281 - val_loss: 0.8783 - val_accuracy: 0.8418\n",
      "Epoch 48/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.2199 - accuracy: 0.9387 - val_loss: 0.8714 - val_accuracy: 0.8447\n",
      "Epoch 49/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.2016 - accuracy: 0.9413 - val_loss: 0.9947 - val_accuracy: 0.8500\n",
      "Epoch 50/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.2213 - accuracy: 0.9366 - val_loss: 1.0245 - val_accuracy: 0.8411\n",
      "Epoch 51/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1833 - accuracy: 0.9516 - val_loss: 1.1459 - val_accuracy: 0.8447\n",
      "Epoch 52/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.1751 - accuracy: 0.9509 - val_loss: 1.2465 - val_accuracy: 0.8165\n",
      "Epoch 53/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.2415 - accuracy: 0.9373 - val_loss: 0.9050 - val_accuracy: 0.8411\n",
      "Epoch 54/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1982 - accuracy: 0.9476 - val_loss: 0.9271 - val_accuracy: 0.8486\n",
      "Epoch 55/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 0.1472 - accuracy: 0.9579 - val_loss: 1.0013 - val_accuracy: 0.8489\n",
      "Epoch 56/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.2269 - accuracy: 0.9406 - val_loss: 0.8237 - val_accuracy: 0.8493\n",
      "Epoch 57/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.1371 - accuracy: 0.9638 - val_loss: 1.0372 - val_accuracy: 0.8507\n",
      "Epoch 58/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1644 - accuracy: 0.9566 - val_loss: 1.1247 - val_accuracy: 0.8436\n",
      "Epoch 59/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1355 - accuracy: 0.9627 - val_loss: 1.0711 - val_accuracy: 0.8489\n",
      "Epoch 60/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1518 - accuracy: 0.9606 - val_loss: 1.1117 - val_accuracy: 0.8415\n",
      "Epoch 61/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1927 - accuracy: 0.9540 - val_loss: 1.0319 - val_accuracy: 0.8375\n",
      "Epoch 62/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.1449 - accuracy: 0.9654 - val_loss: 1.2394 - val_accuracy: 0.8532\n",
      "Epoch 63/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.1227 - accuracy: 0.9682 - val_loss: 1.1271 - val_accuracy: 0.8358\n",
      "Epoch 64/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.1497 - accuracy: 0.9627 - val_loss: 1.1667 - val_accuracy: 0.8472\n",
      "Epoch 65/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.1074 - accuracy: 0.9722 - val_loss: 1.2938 - val_accuracy: 0.8436\n",
      "Epoch 66/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1222 - accuracy: 0.9682 - val_loss: 1.3048 - val_accuracy: 0.8162\n",
      "Epoch 67/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1795 - accuracy: 0.9546 - val_loss: 1.1016 - val_accuracy: 0.8368\n",
      "Epoch 68/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1304 - accuracy: 0.9670 - val_loss: 1.2172 - val_accuracy: 0.8393\n",
      "Epoch 69/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1254 - accuracy: 0.9671 - val_loss: 1.3015 - val_accuracy: 0.8383\n",
      "Epoch 70/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1350 - accuracy: 0.9665 - val_loss: 1.0677 - val_accuracy: 0.8343\n",
      "Epoch 71/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.1043 - accuracy: 0.9732 - val_loss: 1.3981 - val_accuracy: 0.8251\n",
      "Epoch 72/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.0993 - accuracy: 0.9741 - val_loss: 1.3722 - val_accuracy: 0.8486\n",
      "Epoch 73/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.0776 - accuracy: 0.9786 - val_loss: 1.3851 - val_accuracy: 0.8326\n",
      "Epoch 74/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1458 - accuracy: 0.9641 - val_loss: 1.1948 - val_accuracy: 0.8286\n",
      "Epoch 75/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1085 - accuracy: 0.9707 - val_loss: 1.2470 - val_accuracy: 0.8432\n",
      "Epoch 76/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 0.1181 - accuracy: 0.9710 - val_loss: 1.4835 - val_accuracy: 0.8358\n",
      "Epoch 77/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.1377 - accuracy: 0.9651 - val_loss: 1.1894 - val_accuracy: 0.8443\n",
      "Epoch 78/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.0834 - accuracy: 0.9794 - val_loss: 1.5609 - val_accuracy: 0.8536\n",
      "Epoch 79/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.1056 - accuracy: 0.9738 - val_loss: 1.3636 - val_accuracy: 0.8429\n",
      "Epoch 80/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 0.1179 - accuracy: 0.9743 - val_loss: 1.6424 - val_accuracy: 0.8130\n",
      "Epoch 81/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.0898 - accuracy: 0.9780 - val_loss: 1.6308 - val_accuracy: 0.8375\n",
      "Epoch 82/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1027 - accuracy: 0.9727 - val_loss: 1.5021 - val_accuracy: 0.8322\n",
      "Epoch 83/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.0816 - accuracy: 0.9781 - val_loss: 1.4551 - val_accuracy: 0.8301\n",
      "Epoch 84/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1340 - accuracy: 0.9688 - val_loss: 1.4624 - val_accuracy: 0.8461\n",
      "Epoch 85/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.1390 - accuracy: 0.9673 - val_loss: 1.4070 - val_accuracy: 0.8340\n",
      "Epoch 86/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.0845 - accuracy: 0.9774 - val_loss: 1.3952 - val_accuracy: 0.8465\n",
      "Epoch 87/100\n",
      "11227/11227 [==============================] - 9s 780us/sample - loss: 0.0754 - accuracy: 0.9803 - val_loss: 1.4233 - val_accuracy: 0.8461\n",
      "Epoch 88/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1133 - accuracy: 0.9727 - val_loss: 1.3687 - val_accuracy: 0.8358\n",
      "Epoch 89/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1029 - accuracy: 0.9743 - val_loss: 1.5599 - val_accuracy: 0.8347\n",
      "Epoch 90/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.0739 - accuracy: 0.9812 - val_loss: 1.5907 - val_accuracy: 0.8507\n",
      "Epoch 91/100\n",
      "11227/11227 [==============================] - 9s 785us/sample - loss: 0.1244 - accuracy: 0.9694 - val_loss: 1.3983 - val_accuracy: 0.8461\n",
      "Epoch 92/100\n",
      "11227/11227 [==============================] - 9s 781us/sample - loss: 0.0825 - accuracy: 0.9805 - val_loss: 1.3854 - val_accuracy: 0.8425\n",
      "Epoch 93/100\n",
      "11227/11227 [==============================] - 9s 784us/sample - loss: 0.0558 - accuracy: 0.9869 - val_loss: 1.7164 - val_accuracy: 0.8422\n",
      "Epoch 94/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.0654 - accuracy: 0.9835 - val_loss: 1.5841 - val_accuracy: 0.8461\n",
      "Epoch 95/100\n",
      "11227/11227 [==============================] - 9s 782us/sample - loss: 0.0606 - accuracy: 0.9849 - val_loss: 2.2469 - val_accuracy: 0.8069\n",
      "Epoch 96/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.1290 - accuracy: 0.9689 - val_loss: 1.5457 - val_accuracy: 0.8408\n",
      "Epoch 97/100\n",
      "11227/11227 [==============================] - 9s 786us/sample - loss: 0.1106 - accuracy: 0.9737 - val_loss: 1.3633 - val_accuracy: 0.8318\n",
      "Epoch 98/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.0795 - accuracy: 0.9784 - val_loss: 1.4830 - val_accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "11227/11227 [==============================] - 9s 783us/sample - loss: 0.0832 - accuracy: 0.9790 - val_loss: 1.5108 - val_accuracy: 0.8375\n",
      "Epoch 100/100\n",
      "11227/11227 [==============================] - 9s 780us/sample - loss: 0.0522 - accuracy: 0.9871 - val_loss: 1.4477 - val_accuracy: 0.8432\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x157d85e3148>"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_6.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\5x2_3mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_6.save('saved_model\\\\5x2_3mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_15 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_16 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nconv2d_17 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_18 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_19 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nconv2d_20 (Conv2D)           (None, 75, 75, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_6 (MaxPooling2 (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_21 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_22 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nconv2d_23 (Conv2D)           (None, 38, 38, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_24 (Conv2D)           (None, 19, 19, 128)       73856     \n_________________________________________________________________\nconv2d_25 (Conv2D)           (None, 19, 19, 128)       147584    \n_________________________________________________________________\nconv2d_26 (Conv2D)           (None, 19, 19, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 10, 10, 128)       0         \n_________________________________________________________________\nconv2d_27 (Conv2D)           (None, 10, 10, 256)       295168    \n_________________________________________________________________\nconv2d_28 (Conv2D)           (None, 10, 10, 256)       590080    \n_________________________________________________________________\nconv2d_29 (Conv2D)           (None, 10, 10, 256)       590080    \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 5, 5, 256)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 6400)              0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 256)               1638656   \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 128)               32896     \n_________________________________________________________________\ndropout_5 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 32)                0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 6)                 198       \n=================================================================\nTotal params: 3,647,014\nTrainable params: 3,647,014\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_7 = Models.Sequential()\n",
    "\n",
    "model_7.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_7.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_7.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_7.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_7.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_7.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_7.add(Layers.Flatten())\n",
    "model_7.add(Layers.Dense(256,activation='relu'))\n",
    "model_7.add(Layers.Dropout(rate=0.5))\n",
    "model_7.add(Layers.Dense(128,activation='relu'))\n",
    "model_7.add(Layers.Dropout(rate=0.5))\n",
    "model_7.add(Layers.Dense(64,activation='relu'))\n",
    "model_7.add(Layers.Dropout(rate=0.5))\n",
    "model_7.add(Layers.Dense(32,activation='relu'))\n",
    "model_7.add(Layers.Dropout(rate=0.5))\n",
    "model_7.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_7.compile(optimizer=Optimizer.Adam(lr=0.00001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/100\n",
      "11227/11227 [==============================] - 16s 1ms/sample - loss: 1.7734 - accuracy: 0.2019 - val_loss: 1.7416 - val_accuracy: 0.4254\n",
      "Epoch 2/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.7489 - accuracy: 0.2250 - val_loss: 1.6917 - val_accuracy: 0.4361\n",
      "Epoch 3/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.7282 - accuracy: 0.2382 - val_loss: 1.6237 - val_accuracy: 0.4507\n",
      "Epoch 4/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.6995 - accuracy: 0.2604 - val_loss: 1.6024 - val_accuracy: 0.4564\n",
      "Epoch 5/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.6827 - accuracy: 0.2700 - val_loss: 1.5627 - val_accuracy: 0.4852\n",
      "Epoch 6/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.6566 - accuracy: 0.2909 - val_loss: 1.5109 - val_accuracy: 0.5020\n",
      "Epoch 7/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.6277 - accuracy: 0.3007 - val_loss: 1.4451 - val_accuracy: 0.5012\n",
      "Epoch 8/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.6019 - accuracy: 0.3158 - val_loss: 1.3987 - val_accuracy: 0.5319\n",
      "Epoch 9/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.5805 - accuracy: 0.3355 - val_loss: 1.3867 - val_accuracy: 0.5522\n",
      "Epoch 10/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.5573 - accuracy: 0.3474 - val_loss: 1.3287 - val_accuracy: 0.5725\n",
      "Epoch 11/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.5311 - accuracy: 0.3547 - val_loss: 1.3071 - val_accuracy: 0.5782\n",
      "Epoch 12/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.5173 - accuracy: 0.3681 - val_loss: 1.2699 - val_accuracy: 0.6042\n",
      "Epoch 13/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.4855 - accuracy: 0.3738 - val_loss: 1.2320 - val_accuracy: 0.6192\n",
      "Epoch 14/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.4794 - accuracy: 0.3825 - val_loss: 1.2978 - val_accuracy: 0.5461\n",
      "Epoch 15/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.4691 - accuracy: 0.3905 - val_loss: 1.2544 - val_accuracy: 0.5942\n",
      "Epoch 16/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.4328 - accuracy: 0.4044 - val_loss: 1.1727 - val_accuracy: 0.6110\n",
      "Epoch 17/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.4218 - accuracy: 0.4102 - val_loss: 1.1763 - val_accuracy: 0.6295\n",
      "Epoch 18/100\n",
      "11227/11227 [==============================] - 13s 1ms/sample - loss: 1.4117 - accuracy: 0.4168 - val_loss: 1.1490 - val_accuracy: 0.6142\n",
      "Epoch 19/100\n",
      "11227/11227 [==============================] - 13s 1ms/sample - loss: 1.3932 - accuracy: 0.4192 - val_loss: 1.1989 - val_accuracy: 0.6145\n",
      "Epoch 20/100\n",
      "11227/11227 [==============================] - 13s 1ms/sample - loss: 1.3876 - accuracy: 0.4249 - val_loss: 1.0911 - val_accuracy: 0.6370\n",
      "Epoch 21/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.3675 - accuracy: 0.4364 - val_loss: 1.0753 - val_accuracy: 0.6373\n",
      "Epoch 22/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.3544 - accuracy: 0.4402 - val_loss: 1.0730 - val_accuracy: 0.6338\n",
      "Epoch 23/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.3375 - accuracy: 0.4426 - val_loss: 1.0598 - val_accuracy: 0.6619\n",
      "Epoch 24/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.3407 - accuracy: 0.4459 - val_loss: 1.0802 - val_accuracy: 0.6373\n",
      "Epoch 25/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.3179 - accuracy: 0.4572 - val_loss: 1.0235 - val_accuracy: 0.6352\n",
      "Epoch 26/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.3079 - accuracy: 0.4589 - val_loss: 1.0031 - val_accuracy: 0.6437\n",
      "Epoch 27/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2875 - accuracy: 0.4642 - val_loss: 0.9856 - val_accuracy: 0.6484\n",
      "Epoch 28/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2821 - accuracy: 0.4706 - val_loss: 0.9859 - val_accuracy: 0.6651\n",
      "Epoch 29/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2708 - accuracy: 0.4716 - val_loss: 0.9624 - val_accuracy: 0.6705\n",
      "Epoch 30/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2749 - accuracy: 0.4731 - val_loss: 0.9838 - val_accuracy: 0.6559\n",
      "Epoch 31/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2468 - accuracy: 0.4843 - val_loss: 0.9698 - val_accuracy: 0.6584\n",
      "Epoch 32/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2458 - accuracy: 0.4933 - val_loss: 0.9885 - val_accuracy: 0.6641\n",
      "Epoch 33/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2258 - accuracy: 0.5002 - val_loss: 0.9180 - val_accuracy: 0.6705\n",
      "Epoch 34/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2367 - accuracy: 0.4878 - val_loss: 0.9541 - val_accuracy: 0.6591\n",
      "Epoch 35/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2298 - accuracy: 0.4976 - val_loss: 0.9257 - val_accuracy: 0.6719\n",
      "Epoch 36/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.2141 - accuracy: 0.5040 - val_loss: 0.9083 - val_accuracy: 0.6712\n",
      "Epoch 37/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.1964 - accuracy: 0.5070 - val_loss: 0.8907 - val_accuracy: 0.6755\n",
      "Epoch 38/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.1866 - accuracy: 0.5186 - val_loss: 0.8964 - val_accuracy: 0.6747\n",
      "Epoch 39/100\n",
      "11227/11227 [==============================] - 13s 1ms/sample - loss: 1.1803 - accuracy: 0.5113 - val_loss: 0.9008 - val_accuracy: 0.6676\n",
      "Epoch 40/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1696 - accuracy: 0.5210 - val_loss: 0.8847 - val_accuracy: 0.6694\n",
      "Epoch 41/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1663 - accuracy: 0.5139 - val_loss: 0.8477 - val_accuracy: 0.6933\n",
      "Epoch 42/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1514 - accuracy: 0.5295 - val_loss: 0.8554 - val_accuracy: 0.6840\n",
      "Epoch 43/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1515 - accuracy: 0.5311 - val_loss: 0.8658 - val_accuracy: 0.6893\n",
      "Epoch 44/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1483 - accuracy: 0.5289 - val_loss: 0.8418 - val_accuracy: 0.6886\n",
      "Epoch 45/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1361 - accuracy: 0.5332 - val_loss: 0.8653 - val_accuracy: 0.6822\n",
      "Epoch 46/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1265 - accuracy: 0.5408 - val_loss: 0.8560 - val_accuracy: 0.6993\n",
      "Epoch 47/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1187 - accuracy: 0.5361 - val_loss: 0.8389 - val_accuracy: 0.6943\n",
      "Epoch 48/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1117 - accuracy: 0.5449 - val_loss: 0.8375 - val_accuracy: 0.6908\n",
      "Epoch 49/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.1077 - accuracy: 0.5560 - val_loss: 0.8178 - val_accuracy: 0.6801\n",
      "Epoch 50/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.0887 - accuracy: 0.5580 - val_loss: 0.8226 - val_accuracy: 0.7022\n",
      "Epoch 51/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.0881 - accuracy: 0.5593 - val_loss: 0.8077 - val_accuracy: 0.7043\n",
      "Epoch 52/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.0798 - accuracy: 0.5591 - val_loss: 0.8110 - val_accuracy: 0.7086\n",
      "Epoch 53/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.0750 - accuracy: 0.5597 - val_loss: 0.8073 - val_accuracy: 0.6961\n",
      "Epoch 54/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.0726 - accuracy: 0.5654 - val_loss: 0.7951 - val_accuracy: 0.6865\n",
      "Epoch 55/100\n",
      "11227/11227 [==============================] - 14s 1ms/sample - loss: 1.0609 - accuracy: 0.5717 - val_loss: 0.7779 - val_accuracy: 0.7132\n",
      "Epoch 56/100\n",
      "11227/11227 [==============================] - 13s 1ms/sample - loss: 1.0428 - accuracy: 0.5788 - val_loss: 0.8020 - val_accuracy: 0.7111\n",
      "Epoch 57/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0379 - accuracy: 0.5812 - val_loss: 0.7720 - val_accuracy: 0.7054\n",
      "Epoch 58/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0459 - accuracy: 0.5744 - val_loss: 0.7704 - val_accuracy: 0.7161\n",
      "Epoch 59/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0400 - accuracy: 0.5801 - val_loss: 0.7521 - val_accuracy: 0.7225\n",
      "Epoch 60/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0181 - accuracy: 0.5826 - val_loss: 0.7625 - val_accuracy: 0.7200\n",
      "Epoch 61/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0178 - accuracy: 0.5891 - val_loss: 0.8886 - val_accuracy: 0.6929\n",
      "Epoch 62/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0252 - accuracy: 0.5864 - val_loss: 0.7740 - val_accuracy: 0.7143\n",
      "Epoch 63/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 1.0014 - accuracy: 0.5985 - val_loss: 0.7488 - val_accuracy: 0.7321\n",
      "Epoch 64/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9963 - accuracy: 0.5998 - val_loss: 0.7448 - val_accuracy: 0.7164\n",
      "Epoch 65/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9979 - accuracy: 0.5990 - val_loss: 0.7320 - val_accuracy: 0.7022\n",
      "Epoch 66/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9886 - accuracy: 0.5989 - val_loss: 0.7361 - val_accuracy: 0.7221\n",
      "Epoch 67/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9811 - accuracy: 0.6059 - val_loss: 0.7312 - val_accuracy: 0.7328\n",
      "Epoch 68/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9722 - accuracy: 0.6089 - val_loss: 0.7275 - val_accuracy: 0.7392\n",
      "Epoch 69/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9618 - accuracy: 0.6154 - val_loss: 0.7206 - val_accuracy: 0.7342\n",
      "Epoch 70/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9705 - accuracy: 0.6092 - val_loss: 0.7212 - val_accuracy: 0.7435\n",
      "Epoch 71/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9516 - accuracy: 0.6182 - val_loss: 0.7074 - val_accuracy: 0.7467\n",
      "Epoch 72/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9491 - accuracy: 0.6227 - val_loss: 0.6978 - val_accuracy: 0.7574\n",
      "Epoch 73/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9418 - accuracy: 0.6199 - val_loss: 0.6891 - val_accuracy: 0.7399\n",
      "Epoch 74/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9246 - accuracy: 0.6324 - val_loss: 0.7167 - val_accuracy: 0.7631\n",
      "Epoch 75/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9278 - accuracy: 0.6279 - val_loss: 0.6724 - val_accuracy: 0.7463\n",
      "Epoch 76/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9288 - accuracy: 0.6248 - val_loss: 0.7017 - val_accuracy: 0.7478\n",
      "Epoch 77/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9133 - accuracy: 0.6388 - val_loss: 0.6722 - val_accuracy: 0.7513\n",
      "Epoch 78/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9095 - accuracy: 0.6410 - val_loss: 0.7032 - val_accuracy: 0.7488\n",
      "Epoch 79/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9128 - accuracy: 0.6399 - val_loss: 0.6885 - val_accuracy: 0.7581\n",
      "Epoch 80/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.9055 - accuracy: 0.6427 - val_loss: 0.6763 - val_accuracy: 0.7392\n",
      "Epoch 81/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8920 - accuracy: 0.6500 - val_loss: 0.7623 - val_accuracy: 0.7535\n",
      "Epoch 82/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8914 - accuracy: 0.6483 - val_loss: 0.6844 - val_accuracy: 0.7645\n",
      "Epoch 83/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8748 - accuracy: 0.6492 - val_loss: 0.6795 - val_accuracy: 0.7713\n",
      "Epoch 84/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8877 - accuracy: 0.6472 - val_loss: 0.6635 - val_accuracy: 0.7713\n",
      "Epoch 85/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8731 - accuracy: 0.6551 - val_loss: 0.6831 - val_accuracy: 0.7684\n",
      "Epoch 86/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8606 - accuracy: 0.6542 - val_loss: 0.6714 - val_accuracy: 0.7688\n",
      "Epoch 87/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8619 - accuracy: 0.6549 - val_loss: 0.6832 - val_accuracy: 0.7731\n",
      "Epoch 88/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8734 - accuracy: 0.6573 - val_loss: 0.6395 - val_accuracy: 0.7756\n",
      "Epoch 89/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8462 - accuracy: 0.6659 - val_loss: 0.6362 - val_accuracy: 0.7738\n",
      "Epoch 90/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8518 - accuracy: 0.6629 - val_loss: 0.6365 - val_accuracy: 0.7898\n",
      "Epoch 91/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8498 - accuracy: 0.6616 - val_loss: 0.6474 - val_accuracy: 0.7848\n",
      "Epoch 92/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8337 - accuracy: 0.6697 - val_loss: 0.6402 - val_accuracy: 0.7788\n",
      "Epoch 93/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8380 - accuracy: 0.6709 - val_loss: 0.6456 - val_accuracy: 0.7984\n",
      "Epoch 94/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8315 - accuracy: 0.6722 - val_loss: 0.6599 - val_accuracy: 0.7927\n",
      "Epoch 95/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8397 - accuracy: 0.6736 - val_loss: 0.6252 - val_accuracy: 0.7702\n",
      "Epoch 96/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8158 - accuracy: 0.6820 - val_loss: 0.6655 - val_accuracy: 0.7805\n",
      "Epoch 97/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8135 - accuracy: 0.6839 - val_loss: 0.6609 - val_accuracy: 0.7599\n",
      "Epoch 98/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8073 - accuracy: 0.6895 - val_loss: 0.6362 - val_accuracy: 0.7927\n",
      "Epoch 99/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8064 - accuracy: 0.6838 - val_loss: 0.6265 - val_accuracy: 0.8055\n",
      "Epoch 100/100\n",
      "11227/11227 [==============================] - 12s 1ms/sample - loss: 0.8017 - accuracy: 0.6896 - val_loss: 0.6109 - val_accuracy: 0.8101\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x280ba71b688>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_7.fit(Images, Labels, \n",
    "                    batch_size=64, \n",
    "                    epochs=100, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\5x3_3mil_Relu\\assets\n"
     ]
    }
   ],
   "source": [
    "model_7.save('saved_model\\\\5x3_3mil_Relu_lowlern')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_72 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_73 (Conv2D)           (None, 150, 150, 16)      2320      \n_________________________________________________________________\nconv2d_74 (Conv2D)           (None, 150, 150, 32)      4640      \n_________________________________________________________________\nconv2d_75 (Conv2D)           (None, 150, 150, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_33 (MaxPooling (None, 75, 75, 32)        0         \n_________________________________________________________________\nconv2d_76 (Conv2D)           (None, 75, 75, 64)        18496     \n_________________________________________________________________\nconv2d_77 (Conv2D)           (None, 75, 75, 64)        36928     \n_________________________________________________________________\nconv2d_78 (Conv2D)           (None, 75, 75, 128)       73856     \n_________________________________________________________________\nconv2d_79 (Conv2D)           (None, 75, 75, 128)       147584    \n_________________________________________________________________\nmax_pooling2d_34 (MaxPooling (None, 38, 38, 128)       0         \n_________________________________________________________________\nconv2d_80 (Conv2D)           (None, 38, 38, 256)       295168    \n_________________________________________________________________\nconv2d_81 (Conv2D)           (None, 38, 38, 256)       590080    \n_________________________________________________________________\nconv2d_82 (Conv2D)           (None, 38, 38, 384)       885120    \n_________________________________________________________________\nconv2d_83 (Conv2D)           (None, 38, 38, 384)       1327488   \n_________________________________________________________________\nmax_pooling2d_35 (MaxPooling (None, 19, 19, 384)       0         \n_________________________________________________________________\nconv2d_84 (Conv2D)           (None, 19, 19, 512)       1769984   \n_________________________________________________________________\nconv2d_85 (Conv2D)           (None, 19, 19, 512)       2359808   \n_________________________________________________________________\nmax_pooling2d_36 (MaxPooling (None, 10, 10, 512)       0         \n_________________________________________________________________\nflatten_9 (Flatten)          (None, 51200)             0         \n_________________________________________________________________\ndense_33 (Dense)             (None, 512)               26214912  \n_________________________________________________________________\ndropout_22 (Dropout)         (None, 512)               0         \n_________________________________________________________________\ndense_34 (Dense)             (None, 384)               196992    \n_________________________________________________________________\ndropout_23 (Dropout)         (None, 384)               0         \n_________________________________________________________________\ndense_35 (Dense)             (None, 256)               98560     \n_________________________________________________________________\ndropout_24 (Dropout)         (None, 256)               0         \n_________________________________________________________________\ndense_36 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndropout_25 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_37 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_26 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_38 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_27 (Dropout)         (None, 32)                0         \n_________________________________________________________________\ndense_39 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 34,075,062\nTrainable params: 34,075,062\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nLargest convolution model, 19Mil params\\nlooks like it starts over fitting around 75 epochs, but valadation accuracy continued to increase\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "model_8 = Models.Sequential()\n",
    "\n",
    "model_8.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_8.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_8.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(384,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(384,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_8.add(Layers.Conv2D(512,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.Conv2D(512,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_8.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_8.add(Layers.Flatten())\n",
    "\n",
    "model_8.add(Layers.Dense(512,activation='relu'))\n",
    "model_8.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_8.add(Layers.Dense(384,activation='relu'))\n",
    "model_8.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_8.add(Layers.Dense(256,activation='relu'))\n",
    "model_8.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_8.add(Layers.Dense(128,activation='relu'))\n",
    "model_8.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_8.add(Layers.Dense(64,activation='relu'))\n",
    "model_8.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_8.add(Layers.Dense(32,activation='relu'))\n",
    "model_8.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_8.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_8.compile(optimizer=Optimizer.Adam(lr=0.00001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_8.summary()\n",
    "'''\n",
    "Largest convolution model, 19Mil params\n",
    "looks like it starts over fitting around 75 epochs, but valadation accuracy continued to increase\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=======================] - 49s 4ms/sample - loss: 1.1935 - accuracy: 0.4998 - val_loss: 1.1882 - val_accuracy: 0.5550\n",
      "Epoch 70/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1859 - accuracy: 0.5027 - val_loss: 1.1707 - val_accuracy: 0.5618\n",
      "Epoch 71/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1715 - accuracy: 0.5007 - val_loss: 1.1710 - val_accuracy: 0.5590\n",
      "Epoch 72/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1621 - accuracy: 0.5137 - val_loss: 1.1939 - val_accuracy: 0.5615\n",
      "Epoch 73/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1553 - accuracy: 0.5087 - val_loss: 1.1722 - val_accuracy: 0.5704\n",
      "Epoch 74/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1511 - accuracy: 0.5137 - val_loss: 1.1587 - val_accuracy: 0.5586\n",
      "Epoch 75/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1463 - accuracy: 0.5129 - val_loss: 1.1830 - val_accuracy: 0.5625\n",
      "Epoch 76/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1296 - accuracy: 0.5207 - val_loss: 1.1526 - val_accuracy: 0.5764\n",
      "Epoch 77/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1250 - accuracy: 0.5247 - val_loss: 1.1529 - val_accuracy: 0.5771\n",
      "Epoch 78/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.1221 - accuracy: 0.5164 - val_loss: 1.1433 - val_accuracy: 0.5750\n",
      "Epoch 79/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0972 - accuracy: 0.5254 - val_loss: 1.1403 - val_accuracy: 0.5775\n",
      "Epoch 80/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0911 - accuracy: 0.5317 - val_loss: 1.1513 - val_accuracy: 0.5611\n",
      "Epoch 81/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0910 - accuracy: 0.5276 - val_loss: 1.1316 - val_accuracy: 0.5632\n",
      "Epoch 82/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0705 - accuracy: 0.5373 - val_loss: 1.1182 - val_accuracy: 0.5693\n",
      "Epoch 83/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0558 - accuracy: 0.5425 - val_loss: 1.1307 - val_accuracy: 0.5746\n",
      "Epoch 84/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0671 - accuracy: 0.5385 - val_loss: 1.1326 - val_accuracy: 0.5696\n",
      "Epoch 85/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0519 - accuracy: 0.5486 - val_loss: 1.1305 - val_accuracy: 0.5575\n",
      "Epoch 86/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0496 - accuracy: 0.5450 - val_loss: 1.0993 - val_accuracy: 0.5739\n",
      "Epoch 87/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0287 - accuracy: 0.5525 - val_loss: 1.1046 - val_accuracy: 0.5700\n",
      "Epoch 88/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0149 - accuracy: 0.5596 - val_loss: 1.1056 - val_accuracy: 0.5761\n",
      "Epoch 89/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0127 - accuracy: 0.5591 - val_loss: 1.1130 - val_accuracy: 0.5668\n",
      "Epoch 90/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 1.0055 - accuracy: 0.5643 - val_loss: 1.0939 - val_accuracy: 0.5825\n",
      "Epoch 91/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9888 - accuracy: 0.5618 - val_loss: 1.1380 - val_accuracy: 0.5575\n",
      "Epoch 92/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9875 - accuracy: 0.5692 - val_loss: 1.0652 - val_accuracy: 0.5707\n",
      "Epoch 93/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9664 - accuracy: 0.5793 - val_loss: 1.1246 - val_accuracy: 0.5786\n",
      "Epoch 94/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9551 - accuracy: 0.5790 - val_loss: 1.1541 - val_accuracy: 0.5750\n",
      "Epoch 95/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9625 - accuracy: 0.5863 - val_loss: 1.1237 - val_accuracy: 0.5768\n",
      "Epoch 96/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9549 - accuracy: 0.5785 - val_loss: 1.1141 - val_accuracy: 0.5761\n",
      "Epoch 97/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9303 - accuracy: 0.5916 - val_loss: 1.1369 - val_accuracy: 0.5686\n",
      "Epoch 98/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9287 - accuracy: 0.5862 - val_loss: 1.1114 - val_accuracy: 0.5778\n",
      "Epoch 99/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9201 - accuracy: 0.5926 - val_loss: 1.1353 - val_accuracy: 0.5803\n",
      "Epoch 100/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.9147 - accuracy: 0.5934 - val_loss: 1.0440 - val_accuracy: 0.5800\n",
      "Epoch 101/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8981 - accuracy: 0.5967 - val_loss: 1.1095 - val_accuracy: 0.5729\n",
      "Epoch 102/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8983 - accuracy: 0.5974 - val_loss: 1.0970 - val_accuracy: 0.5771\n",
      "Epoch 103/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8900 - accuracy: 0.6056 - val_loss: 1.0597 - val_accuracy: 0.5807\n",
      "Epoch 104/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8859 - accuracy: 0.5978 - val_loss: 1.1184 - val_accuracy: 0.5753\n",
      "Epoch 105/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8813 - accuracy: 0.6028 - val_loss: 1.0737 - val_accuracy: 0.5743\n",
      "Epoch 106/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8573 - accuracy: 0.6068 - val_loss: 1.1775 - val_accuracy: 0.5857\n",
      "Epoch 107/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8509 - accuracy: 0.6156 - val_loss: 1.1244 - val_accuracy: 0.5771\n",
      "Epoch 108/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8566 - accuracy: 0.6118 - val_loss: 1.1729 - val_accuracy: 0.5825\n",
      "Epoch 109/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8392 - accuracy: 0.6133 - val_loss: 1.1596 - val_accuracy: 0.5835\n",
      "Epoch 110/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8400 - accuracy: 0.6161 - val_loss: 1.1390 - val_accuracy: 0.5846\n",
      "Epoch 111/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8182 - accuracy: 0.6206 - val_loss: 1.1356 - val_accuracy: 0.5803\n",
      "Epoch 112/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8127 - accuracy: 0.6234 - val_loss: 1.1588 - val_accuracy: 0.5821\n",
      "Epoch 113/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8159 - accuracy: 0.6157 - val_loss: 1.1018 - val_accuracy: 0.5764\n",
      "Epoch 114/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8067 - accuracy: 0.6276 - val_loss: 1.1651 - val_accuracy: 0.5789\n",
      "Epoch 115/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8206 - accuracy: 0.6215 - val_loss: 1.3142 - val_accuracy: 0.5675\n",
      "Epoch 116/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7956 - accuracy: 0.6297 - val_loss: 1.1064 - val_accuracy: 0.5818\n",
      "Epoch 117/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.8105 - accuracy: 0.6217 - val_loss: 1.2079 - val_accuracy: 0.5789\n",
      "Epoch 118/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7760 - accuracy: 0.6340 - val_loss: 1.1297 - val_accuracy: 0.5597\n",
      "Epoch 119/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7709 - accuracy: 0.6292 - val_loss: 1.0711 - val_accuracy: 0.5803\n",
      "Epoch 120/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7728 - accuracy: 0.6317 - val_loss: 1.1352 - val_accuracy: 0.5843\n",
      "Epoch 121/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7683 - accuracy: 0.6346 - val_loss: 1.2245 - val_accuracy: 0.5707\n",
      "Epoch 122/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7798 - accuracy: 0.6327 - val_loss: 1.2153 - val_accuracy: 0.5853\n",
      "Epoch 123/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7816 - accuracy: 0.6305 - val_loss: 1.2603 - val_accuracy: 0.5664\n",
      "Epoch 124/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7587 - accuracy: 0.6396 - val_loss: 1.0914 - val_accuracy: 0.5828\n",
      "Epoch 125/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7793 - accuracy: 0.6263 - val_loss: 1.1581 - val_accuracy: 0.5639\n",
      "Epoch 126/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7479 - accuracy: 0.6377 - val_loss: 1.2046 - val_accuracy: 0.5853\n",
      "Epoch 127/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7525 - accuracy: 0.6348 - val_loss: 1.1157 - val_accuracy: 0.5700\n",
      "Epoch 128/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7529 - accuracy: 0.6373 - val_loss: 1.2117 - val_accuracy: 0.5803\n",
      "Epoch 129/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7394 - accuracy: 0.6431 - val_loss: 1.1261 - val_accuracy: 0.5725\n",
      "Epoch 130/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7467 - accuracy: 0.6422 - val_loss: 1.1206 - val_accuracy: 0.5743\n",
      "Epoch 131/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7568 - accuracy: 0.6413 - val_loss: 1.1071 - val_accuracy: 0.5675\n",
      "Epoch 132/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7300 - accuracy: 0.6463 - val_loss: 1.3329 - val_accuracy: 0.5800\n",
      "Epoch 133/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7324 - accuracy: 0.6416 - val_loss: 1.2549 - val_accuracy: 0.5768\n",
      "Epoch 134/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7293 - accuracy: 0.6408 - val_loss: 1.2591 - val_accuracy: 0.5853\n",
      "Epoch 135/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7143 - accuracy: 0.6462 - val_loss: 1.3213 - val_accuracy: 0.5786\n",
      "Epoch 136/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7303 - accuracy: 0.6417 - val_loss: 1.2537 - val_accuracy: 0.5796\n",
      "Epoch 137/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7132 - accuracy: 0.6480 - val_loss: 1.1960 - val_accuracy: 0.5757\n",
      "Epoch 138/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7177 - accuracy: 0.6445 - val_loss: 1.3128 - val_accuracy: 0.5771\n",
      "Epoch 139/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7108 - accuracy: 0.6511 - val_loss: 1.1074 - val_accuracy: 0.5711\n",
      "Epoch 140/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7075 - accuracy: 0.6484 - val_loss: 1.4262 - val_accuracy: 0.5825\n",
      "Epoch 141/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7329 - accuracy: 0.6411 - val_loss: 1.2198 - val_accuracy: 0.5810\n",
      "Epoch 142/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7157 - accuracy: 0.6511 - val_loss: 1.2450 - val_accuracy: 0.5818\n",
      "Epoch 143/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7043 - accuracy: 0.6527 - val_loss: 1.3373 - val_accuracy: 0.5835\n",
      "Epoch 144/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6984 - accuracy: 0.6524 - val_loss: 1.3872 - val_accuracy: 0.5828\n",
      "Epoch 145/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7142 - accuracy: 0.6491 - val_loss: 1.3289 - val_accuracy: 0.5725\n",
      "Epoch 146/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6965 - accuracy: 0.6542 - val_loss: 1.2659 - val_accuracy: 0.5689\n",
      "Epoch 147/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7055 - accuracy: 0.6506 - val_loss: 1.3396 - val_accuracy: 0.5857\n",
      "Epoch 148/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6853 - accuracy: 0.6527 - val_loss: 1.4038 - val_accuracy: 0.5889\n",
      "Epoch 149/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6800 - accuracy: 0.6564 - val_loss: 1.3222 - val_accuracy: 0.5753\n",
      "Epoch 150/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6963 - accuracy: 0.6545 - val_loss: 1.4275 - val_accuracy: 0.5764\n",
      "Epoch 151/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6875 - accuracy: 0.6540 - val_loss: 1.1931 - val_accuracy: 0.5810\n",
      "Epoch 152/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6854 - accuracy: 0.6540 - val_loss: 1.4212 - val_accuracy: 0.5796\n",
      "Epoch 153/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6866 - accuracy: 0.6499 - val_loss: 1.3666 - val_accuracy: 0.5732\n",
      "Epoch 154/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7075 - accuracy: 0.6483 - val_loss: 1.1945 - val_accuracy: 0.5643\n",
      "Epoch 155/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6833 - accuracy: 0.6548 - val_loss: 1.3931 - val_accuracy: 0.5825\n",
      "Epoch 156/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6830 - accuracy: 0.6558 - val_loss: 1.3032 - val_accuracy: 0.5600\n",
      "Epoch 157/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6853 - accuracy: 0.6542 - val_loss: 1.3344 - val_accuracy: 0.5864\n",
      "Epoch 158/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6673 - accuracy: 0.6561 - val_loss: 1.3288 - val_accuracy: 0.5825\n",
      "Epoch 159/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6871 - accuracy: 0.6540 - val_loss: 1.2030 - val_accuracy: 0.5771\n",
      "Epoch 160/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6753 - accuracy: 0.6527 - val_loss: 1.3973 - val_accuracy: 0.5821\n",
      "Epoch 161/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.7028 - accuracy: 0.6511 - val_loss: 1.2209 - val_accuracy: 0.5725\n",
      "Epoch 162/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6638 - accuracy: 0.6532 - val_loss: 1.4354 - val_accuracy: 0.5846\n",
      "Epoch 163/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6711 - accuracy: 0.6583 - val_loss: 1.2610 - val_accuracy: 0.5778\n",
      "Epoch 164/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6577 - accuracy: 0.6622 - val_loss: 1.5637 - val_accuracy: 0.5892\n",
      "Epoch 165/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6786 - accuracy: 0.6577 - val_loss: 1.4304 - val_accuracy: 0.5850\n",
      "Epoch 166/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6700 - accuracy: 0.6561 - val_loss: 1.2631 - val_accuracy: 0.5771\n",
      "Epoch 167/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6675 - accuracy: 0.6600 - val_loss: 1.2060 - val_accuracy: 0.5761\n",
      "Epoch 168/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6633 - accuracy: 0.6585 - val_loss: 1.6643 - val_accuracy: 0.5803\n",
      "Epoch 169/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6582 - accuracy: 0.6551 - val_loss: 1.2745 - val_accuracy: 0.5818\n",
      "Epoch 170/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6979 - accuracy: 0.6449 - val_loss: 1.2486 - val_accuracy: 0.5761\n",
      "Epoch 171/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6631 - accuracy: 0.6559 - val_loss: 1.5859 - val_accuracy: 0.5825\n",
      "Epoch 172/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6573 - accuracy: 0.6565 - val_loss: 1.6283 - val_accuracy: 0.5825\n",
      "Epoch 173/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6634 - accuracy: 0.6481 - val_loss: 1.6275 - val_accuracy: 0.5768\n",
      "Epoch 174/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6733 - accuracy: 0.6536 - val_loss: 1.3585 - val_accuracy: 0.5860\n",
      "Epoch 175/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6729 - accuracy: 0.6577 - val_loss: 1.2556 - val_accuracy: 0.5803\n",
      "Epoch 176/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6608 - accuracy: 0.6585 - val_loss: 1.3896 - val_accuracy: 0.5821\n",
      "Epoch 177/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6533 - accuracy: 0.6575 - val_loss: 1.5205 - val_accuracy: 0.5846\n",
      "Epoch 178/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6438 - accuracy: 0.6623 - val_loss: 1.6269 - val_accuracy: 0.5835\n",
      "Epoch 179/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6609 - accuracy: 0.6630 - val_loss: 1.4393 - val_accuracy: 0.5771\n",
      "Epoch 180/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6585 - accuracy: 0.6548 - val_loss: 1.3929 - val_accuracy: 0.5771\n",
      "Epoch 181/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6524 - accuracy: 0.6534 - val_loss: 1.2810 - val_accuracy: 0.5597\n",
      "Epoch 182/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6558 - accuracy: 0.6622 - val_loss: 1.4883 - val_accuracy: 0.5875\n",
      "Epoch 183/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6505 - accuracy: 0.6571 - val_loss: 1.5359 - val_accuracy: 0.5839\n",
      "Epoch 184/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6588 - accuracy: 0.6608 - val_loss: 1.4304 - val_accuracy: 0.5611\n",
      "Epoch 185/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6492 - accuracy: 0.6613 - val_loss: 1.4719 - val_accuracy: 0.5835\n",
      "Epoch 186/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6597 - accuracy: 0.6561 - val_loss: 1.3149 - val_accuracy: 0.5761\n",
      "Epoch 187/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6804 - accuracy: 0.6521 - val_loss: 1.4407 - val_accuracy: 0.5764\n",
      "Epoch 188/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6406 - accuracy: 0.6620 - val_loss: 1.3658 - val_accuracy: 0.5860\n",
      "Epoch 189/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6387 - accuracy: 0.6593 - val_loss: 1.3484 - val_accuracy: 0.5775\n",
      "Epoch 190/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6282 - accuracy: 0.6614 - val_loss: 1.6129 - val_accuracy: 0.5771\n",
      "Epoch 191/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6337 - accuracy: 0.6740 - val_loss: 1.6597 - val_accuracy: 0.5533\n",
      "Epoch 192/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6507 - accuracy: 0.6589 - val_loss: 1.3413 - val_accuracy: 0.5800\n",
      "Epoch 193/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6556 - accuracy: 0.6562 - val_loss: 1.5210 - val_accuracy: 0.5704\n",
      "Epoch 194/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6380 - accuracy: 0.6622 - val_loss: 1.2797 - val_accuracy: 0.5629\n",
      "Epoch 195/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6356 - accuracy: 0.6657 - val_loss: 1.5194 - val_accuracy: 0.5889\n",
      "Epoch 196/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6321 - accuracy: 0.6635 - val_loss: 1.4942 - val_accuracy: 0.5828\n",
      "Epoch 197/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6333 - accuracy: 0.6555 - val_loss: 1.6919 - val_accuracy: 0.5871\n",
      "Epoch 198/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6481 - accuracy: 0.6602 - val_loss: 1.6116 - val_accuracy: 0.5835\n",
      "Epoch 199/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6328 - accuracy: 0.6619 - val_loss: 1.5663 - val_accuracy: 0.5864\n",
      "Epoch 200/200\n",
      "11227/11227 [==============================] - 49s 4ms/sample - loss: 0.6542 - accuracy: 0.6584 - val_loss: 1.2545 - val_accuracy: 0.5771\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x157d87bc6c8>"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_8.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=200, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-bd0e6aa95dfa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_8\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'saved_model\\\\4xMixed_34mil_Relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \"\"\"\n\u001b[0;32m   1007\u001b[0m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[1;32m-> 1008\u001b[1;33m                     signatures, options)\n\u001b[0m\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1010\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    113\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n\u001b[1;32m--> 115\u001b[1;33m                           signatures, options)\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(model, filepath, overwrite, include_optimizer, signatures, options)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;31m# we use the default replica context here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mdistribution_strategy_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_default_replica_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m       \u001b[0msave_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minclude_optimizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\saved_model\\save.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, export_dir, signatures, options)\u001b[0m\n\u001b[0;32m    914\u001b[0m   \u001b[1;31m# SavedModel proto itself.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m   \u001b[0mutils_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_or_create_variables_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 916\u001b[1;33m   \u001b[0mobject_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mutils_impl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variables_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    917\u001b[0m   builder_impl.copy_assets_to_destination_dir(asset_info.asset_filename_map,\n\u001b[0;32m    918\u001b[0m                                               export_dir)\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecursive_create_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     save_path, new_feed_additions = self._save_cached_when_graph_building(\n\u001b[1;32m-> 1168\u001b[1;33m         file_prefix=file_prefix_tensor, object_graph_tensor=object_graph_tensor)\n\u001b[0m\u001b[0;32m   1169\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_feed_additions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\tracking\\util.py\u001b[0m in \u001b[0;36m_save_cached_when_graph_building\u001b[1;34m(self, file_prefix, object_graph_tensor)\u001b[0m\n\u001b[0;32m   1114\u001b[0m         or context.executing_eagerly() or ops.inside_function()):\n\u001b[0;32m   1115\u001b[0m       \u001b[0msaver\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunctional_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMultiDeviceSaver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnamed_saveable_objects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1116\u001b[1;33m       \u001b[0msave_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1117\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msave_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m# _SingleDeviceSaver will use the CPU device when necessary, but initial\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;31m# read operations should be placed on the SaveableObject's device.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[0msharded_saves\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshard_prefix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharded_saves\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\functional_saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix)\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mspec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msaveable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspecs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mtensor_names\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mtensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[0mtensor_slices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cpu:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object.py\u001b[0m in \u001b[0;36mtensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\training\\saving\\saveable_object_util.py\u001b[0m in \u001b[0;36mf\u001b[1;34m()\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;31m# we copy them to CPU on the same machine first.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/device:CPU:0\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m               \u001b[1;32mreturn\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   3824\u001b[0m         \u001b[1;32mpass\u001b[0m  \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3825\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3826\u001b[1;33m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3827\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3828\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\" name: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6605\u001b[0m   \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6606\u001b[1;33m   \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6607\u001b[0m   \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:GPU:0 to /job:localhost/replica:0/task:0/device:CPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]"
     ]
    }
   ],
   "source": [
    "model_8.save('saved_model\\\\4xMixed_34mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_7 (Conv2D)            (None, 150, 150, 16)      448       \n_________________________________________________________________\nmax_pooling2d_7 (MaxPooling2 (None, 75, 75, 16)        0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 75, 75, 32)        4640      \n_________________________________________________________________\nmax_pooling2d_8 (MaxPooling2 (None, 38, 38, 32)        0         \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 38, 38, 64)        18496     \n_________________________________________________________________\nmax_pooling2d_9 (MaxPooling2 (None, 19, 19, 64)        0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 19, 19, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_10 (MaxPooling (None, 10, 10, 128)       0         \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 10, 10, 256)       295168    \n_________________________________________________________________\nmax_pooling2d_11 (MaxPooling (None, 5, 5, 256)         0         \n_________________________________________________________________\nconv2d_12 (Conv2D)           (None, 5, 5, 384)         885120    \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 3, 3, 384)         0         \n_________________________________________________________________\nconv2d_13 (Conv2D)           (None, 3, 3, 512)         1769984   \n_________________________________________________________________\nmax_pooling2d_13 (MaxPooling (None, 2, 2, 512)         0         \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 2048)              0         \n_________________________________________________________________\ndense_7 (Dense)              (None, 512)               1049088   \n_________________________________________________________________\ndropout_6 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_8 (Dense)              (None, 384)               196992    \n_________________________________________________________________\ndropout_7 (Dropout)          (None, 384)               0         \n_________________________________________________________________\ndense_9 (Dense)              (None, 256)               98560     \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 256)               0         \n_________________________________________________________________\ndense_10 (Dense)             (None, 128)               32896     \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 128)               0         \n_________________________________________________________________\ndense_11 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_10 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndense_13 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 4,435,782\nTrainable params: 4,435,782\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_9 = Models.Sequential()\n",
    "\n",
    "model_9.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Conv2D(256,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Conv2D(384,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Conv2D(512,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_9.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_9.add(Layers.Flatten())\n",
    "\n",
    "model_9.add(Layers.Dense(512,activation='relu'))\n",
    "model_9.add(Layers.Dropout(rate=0.5))\n",
    "model_9.add(Layers.Dense(384,activation='relu'))\n",
    "model_9.add(Layers.Dropout(rate=0.5))\n",
    "model_9.add(Layers.Dense(256,activation='relu'))\n",
    "model_9.add(Layers.Dropout(rate=0.5))\n",
    "model_9.add(Layers.Dense(128,activation='relu'))\n",
    "model_9.add(Layers.Dropout(rate=0.5))\n",
    "model_9.add(Layers.Dense(64,activation='relu'))\n",
    "model_9.add(Layers.Dropout(rate=0.5))\n",
    "model_9.add(Layers.Dense(32,activation='relu'))\n",
    "\n",
    "model_9.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_9.compile(optimizer=Optimizer.Adam(lr=0.0001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/200\n",
      "11227/11227 [==============================] - 7s 645us/sample - loss: 1.9871 - accuracy: 0.1680 - val_loss: 1.7905 - val_accuracy: 0.1739\n",
      "Epoch 2/200\n",
      "11227/11227 [==============================] - 6s 560us/sample - loss: 1.7977 - accuracy: 0.1721 - val_loss: 1.7899 - val_accuracy: 0.2476\n",
      "Epoch 3/200\n",
      "11227/11227 [==============================] - 6s 553us/sample - loss: 1.7905 - accuracy: 0.1734 - val_loss: 1.7721 - val_accuracy: 0.2009\n",
      "Epoch 4/200\n",
      "11227/11227 [==============================] - 6s 562us/sample - loss: 1.7257 - accuracy: 0.2242 - val_loss: 1.6532 - val_accuracy: 0.3580\n",
      "Epoch 5/200\n",
      "11227/11227 [==============================] - 6s 566us/sample - loss: 1.6288 - accuracy: 0.3077 - val_loss: 1.5264 - val_accuracy: 0.3912\n",
      "Epoch 6/200\n",
      "11227/11227 [==============================] - 6s 550us/sample - loss: 1.5455 - accuracy: 0.3358 - val_loss: 1.4634 - val_accuracy: 0.4008\n",
      "Epoch 7/200\n",
      "11227/11227 [==============================] - 6s 555us/sample - loss: 1.4760 - accuracy: 0.3504 - val_loss: 1.3964 - val_accuracy: 0.4257\n",
      "Epoch 8/200\n",
      "11227/11227 [==============================] - 6s 558us/sample - loss: 1.3818 - accuracy: 0.3811 - val_loss: 1.2704 - val_accuracy: 0.4393\n",
      "Epoch 9/200\n",
      "11227/11227 [==============================] - 6s 554us/sample - loss: 1.2801 - accuracy: 0.4074 - val_loss: 1.1609 - val_accuracy: 0.4524\n",
      "Epoch 10/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 1.2150 - accuracy: 0.4317 - val_loss: 1.1219 - val_accuracy: 0.4457\n",
      "Epoch 11/200\n",
      "11227/11227 [==============================] - 6s 553us/sample - loss: 1.1742 - accuracy: 0.4627 - val_loss: 1.1345 - val_accuracy: 0.4870\n",
      "Epoch 12/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 1.1075 - accuracy: 0.5071 - val_loss: 0.9707 - val_accuracy: 0.6327\n",
      "Epoch 13/200\n",
      "11227/11227 [==============================] - 6s 561us/sample - loss: 1.0395 - accuracy: 0.5413 - val_loss: 0.9421 - val_accuracy: 0.6056\n",
      "Epoch 14/200\n",
      "11227/11227 [==============================] - 6s 555us/sample - loss: 0.9818 - accuracy: 0.5712 - val_loss: 0.9051 - val_accuracy: 0.6566\n",
      "Epoch 15/200\n",
      "11227/11227 [==============================] - 6s 555us/sample - loss: 0.9195 - accuracy: 0.5964 - val_loss: 0.9059 - val_accuracy: 0.6779\n",
      "Epoch 16/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.8469 - accuracy: 0.6400 - val_loss: 0.8797 - val_accuracy: 0.6804\n",
      "Epoch 17/200\n",
      "11227/11227 [==============================] - 6s 553us/sample - loss: 0.7836 - accuracy: 0.6875 - val_loss: 0.8191 - val_accuracy: 0.7335\n",
      "Epoch 18/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.7089 - accuracy: 0.7306 - val_loss: 0.7961 - val_accuracy: 0.7325\n",
      "Epoch 19/200\n",
      "11227/11227 [==============================] - 6s 554us/sample - loss: 0.6275 - accuracy: 0.7769 - val_loss: 0.7417 - val_accuracy: 0.7905\n",
      "Epoch 20/200\n",
      "11227/11227 [==============================] - 6s 554us/sample - loss: 0.5319 - accuracy: 0.8164 - val_loss: 0.8148 - val_accuracy: 0.7303\n",
      "Epoch 21/200\n",
      "11227/11227 [==============================] - 6s 560us/sample - loss: 0.5024 - accuracy: 0.8342 - val_loss: 0.8991 - val_accuracy: 0.7328\n",
      "Epoch 22/200\n",
      "11227/11227 [==============================] - 6s 561us/sample - loss: 0.4002 - accuracy: 0.8754 - val_loss: 0.8491 - val_accuracy: 0.7823\n",
      "Epoch 23/200\n",
      "11227/11227 [==============================] - 6s 556us/sample - loss: 0.3917 - accuracy: 0.8798 - val_loss: 0.7857 - val_accuracy: 0.7955\n",
      "Epoch 24/200\n",
      "11227/11227 [==============================] - 6s 563us/sample - loss: 0.2973 - accuracy: 0.9132 - val_loss: 0.8703 - val_accuracy: 0.7845\n",
      "Epoch 25/200\n",
      "11227/11227 [==============================] - 6s 558us/sample - loss: 0.2845 - accuracy: 0.9175 - val_loss: 0.8946 - val_accuracy: 0.8005\n",
      "Epoch 26/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.2343 - accuracy: 0.9352 - val_loss: 0.8988 - val_accuracy: 0.8105\n",
      "Epoch 27/200\n",
      "11227/11227 [==============================] - 6s 555us/sample - loss: 0.2028 - accuracy: 0.9439 - val_loss: 0.9570 - val_accuracy: 0.7891\n",
      "Epoch 28/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.2398 - accuracy: 0.9368 - val_loss: 0.9672 - val_accuracy: 0.8016\n",
      "Epoch 29/200\n",
      "11227/11227 [==============================] - 6s 563us/sample - loss: 0.1693 - accuracy: 0.9558 - val_loss: 1.1939 - val_accuracy: 0.8037\n",
      "Epoch 30/200\n",
      "11227/11227 [==============================] - 6s 561us/sample - loss: 0.1754 - accuracy: 0.9550 - val_loss: 1.2279 - val_accuracy: 0.7781\n",
      "Epoch 31/200\n",
      "11227/11227 [==============================] - 6s 556us/sample - loss: 0.1377 - accuracy: 0.9649 - val_loss: 1.0725 - val_accuracy: 0.8048\n",
      "Epoch 32/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.1073 - accuracy: 0.9726 - val_loss: 1.2848 - val_accuracy: 0.8180\n",
      "Epoch 33/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.1565 - accuracy: 0.9654 - val_loss: 1.3765 - val_accuracy: 0.8023\n",
      "Epoch 34/200\n",
      "11227/11227 [==============================] - 6s 560us/sample - loss: 0.1171 - accuracy: 0.9711 - val_loss: 1.2151 - val_accuracy: 0.8176\n",
      "Epoch 35/200\n",
      "11227/11227 [==============================] - 6s 553us/sample - loss: 0.1265 - accuracy: 0.9681 - val_loss: 1.1557 - val_accuracy: 0.8019\n",
      "Epoch 36/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.1044 - accuracy: 0.9762 - val_loss: 1.3578 - val_accuracy: 0.8062\n",
      "Epoch 37/200\n",
      "11227/11227 [==============================] - 6s 558us/sample - loss: 0.0870 - accuracy: 0.9803 - val_loss: 1.2402 - val_accuracy: 0.8051\n",
      "Epoch 38/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.1241 - accuracy: 0.9701 - val_loss: 1.2964 - val_accuracy: 0.8204\n",
      "Epoch 39/200\n",
      "11227/11227 [==============================] - 6s 555us/sample - loss: 0.0986 - accuracy: 0.9775 - val_loss: 1.2393 - val_accuracy: 0.8172\n",
      "Epoch 40/200\n",
      "11227/11227 [==============================] - 6s 564us/sample - loss: 0.1334 - accuracy: 0.9670 - val_loss: 1.0615 - val_accuracy: 0.7973\n",
      "Epoch 41/200\n",
      "11227/11227 [==============================] - 6s 552us/sample - loss: 0.0741 - accuracy: 0.9836 - val_loss: 1.5052 - val_accuracy: 0.7830\n",
      "Epoch 42/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.0738 - accuracy: 0.9821 - val_loss: 1.5421 - val_accuracy: 0.8197\n",
      "Epoch 43/200\n",
      "11227/11227 [==============================] - 6s 555us/sample - loss: 0.0706 - accuracy: 0.9825 - val_loss: 1.6373 - val_accuracy: 0.7934\n",
      "Epoch 44/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.0892 - accuracy: 0.9790 - val_loss: 1.3940 - val_accuracy: 0.7798\n",
      "Epoch 45/200\n",
      "11227/11227 [==============================] - 6s 563us/sample - loss: 0.0444 - accuracy: 0.9903 - val_loss: 1.6448 - val_accuracy: 0.8155\n",
      "Epoch 46/200\n",
      "11227/11227 [==============================] - 6s 558us/sample - loss: 0.1301 - accuracy: 0.9703 - val_loss: 1.2602 - val_accuracy: 0.7895\n",
      "Epoch 47/200\n",
      "11227/11227 [==============================] - 6s 552us/sample - loss: 0.0859 - accuracy: 0.9804 - val_loss: 1.4004 - val_accuracy: 0.7891\n",
      "Epoch 48/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.0676 - accuracy: 0.9851 - val_loss: 1.3449 - val_accuracy: 0.8062\n",
      "Epoch 49/200\n",
      "11227/11227 [==============================] - 6s 567us/sample - loss: 0.0442 - accuracy: 0.9893 - val_loss: 1.6623 - val_accuracy: 0.8019\n",
      "Epoch 50/200\n",
      "11227/11227 [==============================] - 6s 557us/sample - loss: 0.0519 - accuracy: 0.9887 - val_loss: 1.6858 - val_accuracy: 0.8076\n",
      "Epoch 51/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.0924 - accuracy: 0.9800 - val_loss: 1.4055 - val_accuracy: 0.8172\n",
      "Epoch 52/200\n",
      "11227/11227 [==============================] - 6s 563us/sample - loss: 0.0635 - accuracy: 0.9857 - val_loss: 1.5442 - val_accuracy: 0.8016\n",
      "Epoch 53/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.0774 - accuracy: 0.9830 - val_loss: 1.4136 - val_accuracy: 0.8123\n",
      "Epoch 54/200\n",
      "11227/11227 [==============================] - 6s 558us/sample - loss: 0.0492 - accuracy: 0.9885 - val_loss: 1.4826 - val_accuracy: 0.8030\n",
      "Epoch 55/200\n",
      "11227/11227 [==============================] - 6s 553us/sample - loss: 0.0625 - accuracy: 0.9851 - val_loss: 1.7093 - val_accuracy: 0.7781\n",
      "Epoch 56/200\n",
      "11227/11227 [==============================] - 6s 558us/sample - loss: 0.0846 - accuracy: 0.9807 - val_loss: 1.3149 - val_accuracy: 0.8098\n",
      "Epoch 57/200\n",
      "11227/11227 [==============================] - 6s 568us/sample - loss: 0.0466 - accuracy: 0.9896 - val_loss: 1.4061 - val_accuracy: 0.7919\n",
      "Epoch 58/200\n",
      "11227/11227 [==============================] - 6s 560us/sample - loss: 0.0453 - accuracy: 0.9889 - val_loss: 1.6072 - val_accuracy: 0.8137\n",
      "Epoch 59/200\n",
      "11227/11227 [==============================] - 6s 560us/sample - loss: 0.0384 - accuracy: 0.9923 - val_loss: 1.6808 - val_accuracy: 0.8087\n",
      "Epoch 60/200\n",
      "11227/11227 [==============================] - 6s 562us/sample - loss: 0.0530 - accuracy: 0.9866 - val_loss: 1.6238 - val_accuracy: 0.8208\n",
      "Epoch 61/200\n",
      "11227/11227 [==============================] - 6s 561us/sample - loss: 0.0492 - accuracy: 0.9891 - val_loss: 1.3972 - val_accuracy: 0.7823\n",
      "Epoch 62/200\n",
      "11227/11227 [==============================] - 6s 569us/sample - loss: 0.0701 - accuracy: 0.9838 - val_loss: 1.5797 - val_accuracy: 0.8140\n",
      "Epoch 63/200\n",
      "11227/11227 [==============================] - 6s 516us/sample - loss: 0.0391 - accuracy: 0.9921 - val_loss: 1.6032 - val_accuracy: 0.8119\n",
      "Epoch 64/200\n",
      "11227/11227 [==============================] - 6s 504us/sample - loss: 0.0345 - accuracy: 0.9923 - val_loss: 1.6595 - val_accuracy: 0.8347\n",
      "Epoch 65/200\n",
      "11227/11227 [==============================] - 6s 530us/sample - loss: 0.0679 - accuracy: 0.9840 - val_loss: 1.9398 - val_accuracy: 0.7880\n",
      "Epoch 66/200\n",
      "11227/11227 [==============================] - 6s 562us/sample - loss: 0.0934 - accuracy: 0.9784 - val_loss: 1.2927 - val_accuracy: 0.8219\n",
      "Epoch 67/200\n",
      "11227/11227 [==============================] - 6s 567us/sample - loss: 0.0424 - accuracy: 0.9898 - val_loss: 1.4816 - val_accuracy: 0.8119\n",
      "Epoch 68/200\n",
      "11227/11227 [==============================] - 6s 573us/sample - loss: 0.0680 - accuracy: 0.9851 - val_loss: 1.2384 - val_accuracy: 0.8144\n",
      "Epoch 69/200\n",
      "11227/11227 [==============================] - 6s 564us/sample - loss: 0.0318 - accuracy: 0.9924 - val_loss: 1.6969 - val_accuracy: 0.8147\n",
      "Epoch 70/200\n",
      "11227/11227 [==============================] - 6s 559us/sample - loss: 0.0347 - accuracy: 0.9914 - val_loss: 1.6117 - val_accuracy: 0.8240\n",
      "Epoch 71/200\n",
      "11227/11227 [==============================] - 6s 573us/sample - loss: 0.0229 - accuracy: 0.9958 - val_loss: 1.8715 - val_accuracy: 0.8037\n",
      "Epoch 72/200\n",
      "10784/11227 [===========================>..] - ETA: 0s - loss: 0.0668 - accuracy: 0.9851"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f38f3bef7512>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     callbacks=[tensorboard_cb])\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_9.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=200, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9.save('saved_model\\\\7x1_4mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=./my_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_54 (Conv2D)           (None, 150, 150, 16)      448       \n_________________________________________________________________\nconv2d_55 (Conv2D)           (None, 150, 150, 32)      4640      \n_________________________________________________________________\nconv2d_56 (Conv2D)           (None, 150, 150, 64)      18496     \n_________________________________________________________________\nconv2d_57 (Conv2D)           (None, 150, 150, 128)     73856     \n_________________________________________________________________\nmax_pooling2d_12 (MaxPooling (None, 75, 75, 128)       0         \n_________________________________________________________________\nconv2d_58 (Conv2D)           (None, 75, 75, 16)        18448     \n_________________________________________________________________\nconv2d_59 (Conv2D)           (None, 75, 75, 32)        4640      \n_________________________________________________________________\nconv2d_60 (Conv2D)           (None, 75, 75, 64)        18496     \n_________________________________________________________________\nconv2d_61 (Conv2D)           (None, 75, 75, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_13 (MaxPooling (None, 38, 38, 128)       0         \n_________________________________________________________________\nconv2d_62 (Conv2D)           (None, 38, 38, 16)        18448     \n_________________________________________________________________\nconv2d_63 (Conv2D)           (None, 38, 38, 32)        4640      \n_________________________________________________________________\nconv2d_64 (Conv2D)           (None, 38, 38, 64)        18496     \n_________________________________________________________________\nconv2d_65 (Conv2D)           (None, 38, 38, 128)       73856     \n_________________________________________________________________\nmax_pooling2d_14 (MaxPooling (None, 19, 19, 128)       0         \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 46208)             0         \n_________________________________________________________________\ndense_19 (Dense)             (None, 128)               5914752   \n_________________________________________________________________\ndropout_17 (Dropout)         (None, 128)               0         \n_________________________________________________________________\ndense_20 (Dense)             (None, 64)                8256      \n_________________________________________________________________\ndropout_18 (Dropout)         (None, 64)                0         \n_________________________________________________________________\ndense_21 (Dense)             (None, 32)                2080      \n_________________________________________________________________\ndropout_19 (Dropout)         (None, 32)                0         \n_________________________________________________________________\ndense_22 (Dense)             (None, 6)                 198       \n=================================================================\nTotal params: 6,253,606\nTrainable params: 6,253,606\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_10 = Models.Sequential()\n",
    "\n",
    "model_10.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu',input_shape=(150,150,3), padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "\n",
    "model_10.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_10.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "\n",
    "model_10.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "model_10.add(Layers.Conv2D(16,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(32,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(64,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.Conv2D(128,kernel_size=(3,3),activation='relu', padding=\"same\"))\n",
    "model_10.add(Layers.MaxPool2D((2,2), strides=2, padding=\"same\"))\n",
    "\n",
    "\n",
    "model_10.add(Layers.Flatten())\n",
    "\n",
    "\n",
    "model_10.add(Layers.Dense(128,activation='relu'))\n",
    "model_10.add(Layers.Dropout(rate=0.5))\n",
    "model_10.add(Layers.Dense(64,activation='relu'))\n",
    "model_10.add(Layers.Dropout(rate=0.5))\n",
    "model_10.add(Layers.Dense(32,activation='relu'))\n",
    "model_10.add(Layers.Dropout(rate=0.5))\n",
    "\n",
    "model_10.add(Layers.Dense(6,activation='softmax'))\n",
    "\n",
    "model_10.compile(optimizer=Optimizer.Adam(lr=0.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train on 11227 samples, validate on 2807 samples\n",
      "Epoch 1/100\n",
      " 7072/11227 [=================>............] - ETA: 16s - loss: 1.8245 - accuracy: 0.1791"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d7e4cc26cbbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                     \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                     callbacks=[tensorboard_cb])\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m   \u001b[0mconstant_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[1;34m(tensor, partial)\u001b[0m\n\u001b[0;32m    820\u001b[0m   \"\"\"\n\u001b[0;32m    821\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    940\u001b[0m     \"\"\"\n\u001b[0;32m    941\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = Callbacks.TensorBoard(run_logdir)\n",
    "\n",
    "model_10.fit(Images, Labels, \n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_10.save('saved_model\\\\4x5mix_8mil_Relu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}